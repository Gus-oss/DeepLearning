{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b010671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------- Librerias ----------------------------------------------\n",
    "#Librerias tipicas para el análisis de datos\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Librerias para la implementación de pyspark\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\hadoop\"\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Libreraias para la implementación de keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Layer, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acfdf3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sessión de Spark creada\n",
      "  Network timeout: 800s\n",
      "  Worker timeout: 600s\n"
     ]
    }
   ],
   "source": [
    "#-----------------------SparkSession-----------------------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC_Taxi_Spark_DeepLearning\") \\\n",
    "    .master(\"local[8]\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .config(\"spark.python.worker.timeout\", \"600s\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"16\") \\\n",
    "    .config(\"spark.default.parallelism\", \"16\") \\\n",
    "    .config(\"spark.rdd.compress\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\" Sessión de Spark creada\")\n",
    "print(\"  Network timeout: 800s\")\n",
    "print(\"  Worker timeout: 600s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c417af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Cargando dataset\n",
      " Numero de registros: 2,964,624\n",
      " Columnas: 19\n"
     ]
    }
   ],
   "source": [
    "#----------------------Cargar datos-----------------------------------------------\n",
    "DATA_PATH = \"C:/Users/PC/Documents/DocumentosGustavo/Github/Maestria/BigData/nyc-taxi-spark/data/yellow/2024/yellow_tripdata_2024-01.parquet\"\n",
    "\n",
    "print(\"\\n Cargando dataset\")\n",
    "df = spark.read.parquet(DATA_PATH)\n",
    "print(f\" Numero de registros: {df.count():,}\")\n",
    "print(f\" Columnas: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f232ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Esquema del dataset:\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEsquema del dataset:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06500a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeros 5 registros\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2024-01-01 00:57:55|  2024-01-01 01:17:43|              1|         1.72|         1|                 N|         186|          79|           2|       17.7|  1.0|    0.5|       0.0|         0.0|                  1.0|        22.7|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:03:00|  2024-01-01 00:09:36|              1|          1.8|         1|                 N|         140|         236|           1|       10.0|  3.5|    0.5|      3.75|         0.0|                  1.0|       18.75|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:17:06|  2024-01-01 00:35:01|              1|          4.7|         1|                 N|         236|          79|           1|       23.3|  3.5|    0.5|       3.0|         0.0|                  1.0|        31.3|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:36:38|  2024-01-01 00:44:56|              1|          1.4|         1|                 N|          79|         211|           1|       10.0|  3.5|    0.5|       2.0|         0.0|                  1.0|        17.0|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:46:51|  2024-01-01 00:52:57|              1|          0.8|         1|                 N|         211|         148|           1|        7.9|  3.5|    0.5|       3.2|         0.0|                  1.0|        16.1|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"Primeros 5 registros\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b914f4",
   "metadata": {},
   "source": [
    "Para la implementación de esta tarea se elegiran las variables: \n",
    "- trip_distance\n",
    "- passenger_count\n",
    "- tpep_pickup_datetime\n",
    "- fare_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3f27e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando features\n",
      "Registros despues de la limpieza: 2,722,784 registros. \n",
      " Completado en 58.7s.\n"
     ]
    }
   ],
   "source": [
    "#----------------------Feature Engineering----------------------------------------\n",
    "def extract_and_scale_features(row):\n",
    "    trip_distance, passenger_count, datetime, fare_amount = row\n",
    "    \n",
    "    if (trip_distance is None or trip_distance <= 0 or trip_distance >= 100 or\n",
    "        passenger_count is None or passenger_count <= 0 or passenger_count > 6 or\n",
    "        datetime is None or\n",
    "        fare_amount is None or fare_amount <= 0 or fare_amount >= 200):\n",
    "        return None\n",
    "    \"\"\"\n",
    "    trip_distance <= 0: Viajes inválidos (errores de sensor).\n",
    "    trip_distance >= 100: Outliers extremos (probablemente errores).\n",
    "    passenger_count <= 0 or > 6: NYC taxis tienen máximo 5 pasajeros + 1 niño.\n",
    "    fare_amount <= 0 or >= 200: Errores de medición o fraudes.\n",
    "\n",
    "    \"\"\"\n",
    "    hour_value = float(datetime.hour) #hora del día: 0-23\n",
    "    day_of_week = float(datetime.weekday() + 1) #dia de la semana: L-D\n",
    "    \n",
    "    features = [\n",
    "        float((trip_distance - 3.0) / 5.0),\n",
    "        float((passenger_count - 1.5) / 1.0),\n",
    "        float((hour_value - 12.0) / 7.0),\n",
    "        float((day_of_week - 4.0) / 2.0)\n",
    "    ]\n",
    "    \"\"\"\n",
    "    Normalización:  z = (x - μ) / σ, esto por que \n",
    "    Convergencia: Gradientes similares en todas las features.\n",
    "    Velocidad: Adam converge más rápido con datos escalados.\n",
    "    Estabilidad numérica: Evita overflow/underflow.\n",
    "    ReLU: Funciona mejor con datos centrados en 0.\n",
    "\n",
    "    \"\"\"\n",
    "    return (features, float(fare_amount)) #Estructura: Tupla (lista_features, label). Compatible con RDD map-reduce y Keras.\n",
    "\n",
    "print(\"\\nProcesando features\")\n",
    "start = time.time()\n",
    "\n",
    "rdd_features = df.select( #convierte a rdd\n",
    "    \"trip_distance\", \"passenger_count\", \"tpep_pickup_datetime\", \"fare_amount\"\n",
    ").rdd.map(lambda row: (\n",
    "    row.trip_distance, row.passenger_count, row.tpep_pickup_datetime, row.fare_amount # Transformación 1-a-1. Ejecución: Lazy (no se ejecuta hasta una acción).\n",
    ")) #Variables que se eligieron para realizar el proyecto\n",
    "\n",
    "rdd_scaled = rdd_features \\\n",
    "    .map(extract_and_scale_features) \\\n",
    "    .filter(lambda x: x is not None) \\\n",
    "    .repartition(16) \\\n",
    "    .cache()\n",
    "# lambda x:x is not None:  Elimina registros None (inválidos)\n",
    "# .repartition(16): Redistribuir datos en 16 particiones balanceadas\n",
    "total_scaled = rdd_scaled.count()\n",
    "print(f\"Registros despues de la limpieza: {total_scaled:,} registros. \\n Completado en {time.time()-start:.1f}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9e3cc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train: 2,178,475 registros\n",
      " Test: 544,309 registros\n"
     ]
    }
   ],
   "source": [
    "#----------------------División Train/Test----------------------------------------\n",
    "train_rdd, test_rdd = rdd_scaled.randomSplit([0.8, 0.2]) #80% entrenamiento y 20% prueba aleatorio \n",
    "\n",
    "from pyspark import StorageLevel\n",
    "train_rdd = train_rdd.repartition(16).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "test_rdd = test_rdd.repartition(8).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "#StorageLevel.MEMORY_AND_DISK: primer intenta usar memoria, sino cabe utiliza disco\n",
    "\n",
    "train_count = train_rdd.count()\n",
    "test_count = test_rdd.count()\n",
    "\n",
    "print(f\"\\n Train: {train_count:,} registros\")\n",
    "print(f\" Test: {test_count:,} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b5d014",
   "metadata": {},
   "source": [
    "Para este trabajo, utilizaremos \n",
    "$$ E_{ck} = \\frac{1}{N}\\sum_{k=1}^{K} \\sum_{x_{i} \\in C_{k}} \\mu_{kp} \\lvert x_{n} - c_{k} \\rvert^{2}$$\n",
    "La arquitectura a trabajar sera la siguiente: \n",
    "- Input (batch, 4 variables)\n",
    "- CompetitiveLearningLayer: $\\lvert x_{n} - c_{k} \\rvert^{2}$ que son las distancias. Despues $\\mu_{kp} = softmax(-distancia)$ la cual suavisa la competición (batch, 64 neuronas)\n",
    "- Capa oculta 1: 32 neuronas con activacion relu\n",
    "- Capa oculta 2: 16 neuronas con activacion relu\n",
    "- Capa oculta 3:  8 neuronas con activacion relu\n",
    "- Capa salida  :  1 neurona con activacion lineal.\n",
    "- Optimizador: Adam\n",
    "- Función de costo: MSE\n",
    "- Metrica adicional: MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9627ae78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "\n",
      " Modelo con Aprendizaje Competitivo creado\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ competitive_learning_layer      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CompetitiveLearningLayer</span>)      │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ competitive_learning_layer      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mCompetitiveLearningLayer\u001b[0m)      │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m9\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,393</span> (13.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,393\u001b[0m (13.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,201</span> (12.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,201\u001b[0m (12.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#----------------------Capa de Aprendizaje Competitivo--------------------\n",
    "class CompetitiveLearningLayer(Layer):\n",
    "    \"\"\"\n",
    "    Capa de Aprendizaje Competitivo con Promedio Ponderado.\n",
    "\n",
    "    Implementa la función de costo competitivo:\n",
    "        E_ck = (1/N) * sum_k sum_{x_i in C_k} mu_kp * ||x_n - c_k||^2\n",
    "\n",
    "    donde:\n",
    "      - c_k  : centros de cluster (parámetros entrenables)\n",
    "      - mu_kp: grados de membresía (soft-competition via softmax)\n",
    "               mu_kp = softmax(-||x_n - c_k||^2)\n",
    "               Neurona ganadora -> membresía cercana a 1\n",
    "               Neuronas perdedoras -> membresía cercana a 0\n",
    "\n",
    "    Flujo de información:\n",
    "      input (batch, 4) --> distancias (batch, K) --> membresías (batch, K)\n",
    "\n",
    "    Los gradientes del MSE final fluyen hacia atrás actualizando tanto\n",
    "    los centros c_k como los pesos de las capas Dense posteriores.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters, **kwargs):\n",
    "        super(CompetitiveLearningLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n_features = int(input_shape[-1])\n",
    "        # Centros de cluster c_k: forma (K, n_features)\n",
    "        # Inicialización glorot para estabilidad del gradiente\n",
    "        self.cluster_centers = self.add_weight(\n",
    "            name='cluster_centers',\n",
    "            shape=(self.n_clusters, n_features),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        super(CompetitiveLearningLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs: (batch, n_features)\n",
    "\n",
    "        # Expandir dimensiones para broadcast\n",
    "        # x_expanded: (batch, 1, n_features)\n",
    "        # c_expanded: (1, K, n_features)\n",
    "        x_expanded = tf.expand_dims(inputs, axis=1)\n",
    "        c_expanded = tf.expand_dims(self.cluster_centers, axis=0)\n",
    "\n",
    "        # Distancias cuadradas ||x_n - c_k||^2: (batch, K)\n",
    "        distances_sq = tf.reduce_sum(tf.square(x_expanded - c_expanded), axis=-1)\n",
    "\n",
    "        # Membresías mu_kp = softmax(-distancias)\n",
    "        # La neurona mas cercana obtiene la mayor membresía -> competición suave\n",
    "        # El signo negativo invierte: menor distancia = mayor activación\n",
    "        memberships = tf.nn.softmax(-distances_sq, axis=-1)  # (batch, K)\n",
    "\n",
    "        return memberships\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'n_clusters': self.n_clusters})\n",
    "        return config\n",
    "\n",
    "\n",
    "#----------------------Modelo con Aprendizaje Competitivo-----------------\n",
    "def create_model():\n",
    "    \"\"\"\n",
    "    Red neuronal con aprendizaje competitivo con promedio ponderado.\n",
    "\n",
    "    La primera capa Dense(64, relu) es reemplazada por CompetitiveLearningLayer(64)\n",
    "    que implementa competición suave entre K=64 centros de cluster.\n",
    "    El resto de la arquitectura y funciones de costo se conservan.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # --- Capa 1: Aprendizaje Competitivo con Promedio (K=64 clusters) ---\n",
    "        # Reemplaza: Dense(64, activation='relu', input_shape=(4,))\n",
    "        # Parámetros entrenables: 64 * 4 = 256 (centros c_k)\n",
    "        # Salida: vector de 64 membresías mu_kp en [0, 1] con suma = 1\n",
    "        CompetitiveLearningLayer(64, input_shape=(4,)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        # --- Capas de regresión (conservadas del modelo original) ---\n",
    "        Dense(32, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "    # Funciones de costo conservadas: MSE + MAE, optimizador Adam\n",
    "    model.compile(optimizer=Adam(0.001), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "print(\"\\n Modelo con Aprendizaje Competitivo creado\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdbd09d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch size: 4096\n",
      "  Batches/época train: 400\n",
      "  Samples/época: 1,638,400\n"
     ]
    }
   ],
   "source": [
    "#----------------------Generador de batches--------------------------------\n",
    "class RobustRDDBatchGenerator: #Generador que usa toLocalIterator().\n",
    "    \n",
    "    def __init__(self, rdd, batch_size=4096, num_batches_per_epoch=None):\n",
    "        self.rdd = rdd\n",
    "        self.batch_size = batch_size\n",
    "        self.total_samples = rdd.count()\n",
    "        \n",
    "        if num_batches_per_epoch:\n",
    "            self.num_batches = num_batches_per_epoch\n",
    "        else:\n",
    "            self.num_batches = max(1, self.total_samples // batch_size)\n",
    "    \n",
    "    def generate_batches(self, seed=42):\n",
    "        \"\"\"\n",
    "        Genera batches usando toLocalIterator.\n",
    "        \n",
    "        toLocalIterator:\n",
    "        - Itera sobre RDD SIN collect masivo\n",
    "        - No causa timeout\n",
    "        - Procesa partición por partición\n",
    "        - 100% RDD distribuido\n",
    "        \"\"\"\n",
    "        # Sample del RDD\n",
    "        fraction = min(1.0, (self.batch_size * self.num_batches) / self.total_samples)\n",
    "        sampled_rdd = self.rdd.sample(False, fraction, seed=seed)\n",
    "        \n",
    "        # Usar toLocalIterator\n",
    "        batch_data = []\n",
    "        batch_count = 0\n",
    "        \n",
    "        \n",
    "        for item in sampled_rdd.toLocalIterator():\n",
    "            batch_data.append(item)\n",
    "            \n",
    "            # Cuando el batch está lleno, yield\n",
    "            if len(batch_data) >= self.batch_size:\n",
    "                X_batch = np.array([x[0] for x in batch_data], dtype=np.float32)\n",
    "                y_batch = np.array([x[1] for x in batch_data], dtype=np.float32)\n",
    "                \n",
    "                yield X_batch, y_batch\n",
    "                \n",
    "                batch_data = []\n",
    "                batch_count += 1\n",
    "                \n",
    "                # Limitar número de batches\n",
    "                if batch_count >= self.num_batches:\n",
    "                    break\n",
    "        \n",
    "        # Último batch parcial\n",
    "        if batch_data and batch_count < self.num_batches:\n",
    "            X_batch = np.array([x[0] for x in batch_data], dtype=np.float32)\n",
    "            y_batch = np.array([x[1] for x in batch_data], dtype=np.float32)\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "# Configuración\n",
    "BATCH_SIZE = 4096  \n",
    "BATCHES_PER_EPOCH_TRAIN = 400\n",
    "BATCHES_PER_EPOCH_VAL = 20\n",
    "\n",
    "train_generator = RobustRDDBatchGenerator(\n",
    "    train_rdd, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_batches_per_epoch=BATCHES_PER_EPOCH_TRAIN\n",
    ")\n",
    "\n",
    "test_generator = RobustRDDBatchGenerator(\n",
    "    test_rdd,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_batches_per_epoch=BATCHES_PER_EPOCH_VAL\n",
    ")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Batches/época train: {BATCHES_PER_EPOCH_TRAIN}\")\n",
    "print(f\"  Samples/época: {BATCH_SIZE * BATCHES_PER_EPOCH_TRAIN:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92f77e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento.\n",
      "\n",
      "   Configuración:\n",
      "   Épocas: 15\n",
      "   Batches/época: 400\n",
      "   Batch size: 4096\n",
      "\n",
      "  Iniciando el entrenamiento\n",
      "\n",
      "\n",
      "Época 1/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 414.2326\n",
      "  Batch 200/400 - loss: 231.1467\n",
      "  Batch 300/400 - loss: 164.0517\n",
      "  Batch 400/400 - loss: 130.7513\n",
      "\n",
      "    Época 1:\n",
      "     loss: 280.3950 - mae: 10.7906\n",
      "     val_loss: 130.0407 - val_mae: 6.4364\n",
      "     Tiempo: 46.5s\n",
      "\n",
      "Época 2/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 114.1165\n",
      "  Batch 200/400 - loss: 100.8519\n",
      "  Batch 300/400 - loss: 91.3459\n",
      "  Batch 400/400 - loss: 84.0341\n",
      "\n",
      "    Época 2:\n",
      "     loss: 102.1900 - mae: 5.4743\n",
      "     val_loss: 82.8130 - val_mae: 4.7934\n",
      "     Tiempo: 52.4s\n",
      "\n",
      "Época 3/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 77.4253\n",
      "  Batch 200/400 - loss: 72.9523\n",
      "  Batch 300/400 - loss: 69.2689\n",
      "  Batch 400/400 - loss: 66.1566\n",
      "\n",
      "    Época 3:\n",
      "     loss: 73.0493 - mae: 4.4436\n",
      "     val_loss: 65.5358 - val_mae: 4.1692\n",
      "     Tiempo: 56.1s\n",
      "\n",
      "Época 4/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 62.9663\n",
      "  Batch 200/400 - loss: 60.7393\n",
      "  Batch 300/400 - loss: 58.8179\n",
      "  Batch 400/400 - loss: 57.1019\n",
      "\n",
      "    Época 4:\n",
      "     loss: 60.7217 - mae: 3.9894\n",
      "     val_loss: 56.7470 - val_mae: 3.8391\n",
      "     Tiempo: 55.0s\n",
      "\n",
      "Época 5/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 55.2221\n",
      "  Batch 200/400 - loss: 53.8771\n",
      "  Batch 300/400 - loss: 52.6755\n",
      "  Batch 400/400 - loss: 51.5473\n",
      "\n",
      "    Época 5:\n",
      "     loss: 53.8333 - mae: 3.7268\n",
      "     val_loss: 51.3136 - val_mae: 3.6283\n",
      "     Tiempo: 45.9s\n",
      "\n",
      "Época 6/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 50.2874\n",
      "  Batch 200/400 - loss: 49.3540\n",
      "  Batch 300/400 - loss: 48.5105\n",
      "  Batch 400/400 - loss: 47.6964\n",
      "\n",
      "    Época 6:\n",
      "     loss: 49.3154 - mae: 3.5486\n",
      "     val_loss: 47.5319 - val_mae: 3.4761\n",
      "     Tiempo: 48.6s\n",
      "\n",
      "Época 7/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 46.7747\n",
      "  Batch 200/400 - loss: 46.0557\n",
      "  Batch 300/400 - loss: 45.4000\n",
      "  Batch 400/400 - loss: 44.7691\n",
      "\n",
      "    Época 7:\n",
      "     loss: 46.0207 - mae: 3.4134\n",
      "     val_loss: 44.6429 - val_mae: 3.3550\n",
      "     Tiempo: 50.2s\n",
      "\n",
      "Época 8/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 44.0571\n",
      "  Batch 200/400 - loss: 43.5077\n",
      "  Batch 300/400 - loss: 42.9891\n",
      "  Batch 400/400 - loss: 42.4839\n",
      "\n",
      "    Época 8:\n",
      "     loss: 43.4717 - mae: 3.3045\n",
      "     val_loss: 42.3941 - val_mae: 3.2581\n",
      "     Tiempo: 52.0s\n",
      "\n",
      "Época 9/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 41.9424\n",
      "  Batch 200/400 - loss: 41.5027\n",
      "  Batch 300/400 - loss: 41.0965\n",
      "  Batch 400/400 - loss: 40.6943\n",
      "\n",
      "    Época 9:\n",
      "     loss: 41.4771 - mae: 3.2178\n",
      "     val_loss: 40.6183 - val_mae: 3.1799\n",
      "     Tiempo: 44.2s\n",
      "\n",
      "Época 10/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 40.2419\n",
      "  Batch 200/400 - loss: 39.8755\n",
      "  Batch 300/400 - loss: 39.5313\n",
      "  Batch 400/400 - loss: 39.1979\n",
      "\n",
      "    Época 10:\n",
      "     loss: 39.8509 - mae: 3.1462\n",
      "     val_loss: 39.1406 - val_mae: 3.1148\n",
      "     Tiempo: 61.5s\n",
      "\n",
      "Época 11/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 38.8405\n",
      "  Batch 200/400 - loss: 38.5379\n",
      "  Batch 300/400 - loss: 38.2592\n",
      "  Batch 400/400 - loss: 37.9739\n",
      "\n",
      "    Época 11:\n",
      "     loss: 38.5183 - mae: 3.0867\n",
      "     val_loss: 37.9312 - val_mae: 3.0601\n",
      "     Tiempo: 50.8s\n",
      "\n",
      "Época 12/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 37.6692\n",
      "  Batch 200/400 - loss: 37.4117\n",
      "  Batch 300/400 - loss: 37.1607\n",
      "  Batch 400/400 - loss: 36.9132\n",
      "\n",
      "    Época 12:\n",
      "     loss: 37.3914 - mae: 3.0357\n",
      "     val_loss: 36.8722 - val_mae: 3.0124\n",
      "     Tiempo: 50.7s\n",
      "\n",
      "Época 13/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 36.6454\n",
      "  Batch 200/400 - loss: 36.4296\n",
      "  Batch 300/400 - loss: 36.2209\n",
      "  Batch 400/400 - loss: 36.0028\n",
      "\n",
      "    Época 13:\n",
      "     loss: 36.4113 - mae: 2.9912\n",
      "     val_loss: 35.9719 - val_mae: 2.9712\n",
      "     Tiempo: 51.3s\n",
      "\n",
      "Época 14/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 35.7842\n",
      "  Batch 200/400 - loss: 35.5938\n",
      "  Batch 300/400 - loss: 35.4110\n",
      "  Batch 400/400 - loss: 35.2238\n",
      "\n",
      "    Época 14:\n",
      "     loss: 35.5786 - mae: 2.9528\n",
      "     val_loss: 35.1978 - val_mae: 2.9351\n",
      "     Tiempo: 48.3s\n",
      "\n",
      "Época 15/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 35.0316\n",
      "  Batch 200/400 - loss: 34.8629\n",
      "  Batch 300/400 - loss: 34.7013\n",
      "  Batch 400/400 - loss: 34.5344\n",
      "\n",
      "    Época 15:\n",
      "     loss: 34.8489 - mae: 2.9185\n",
      "     val_loss: 34.5122 - val_mae: 2.9026\n",
      "     Tiempo: 50.2s\n",
      "\n",
      "================================================================================\n",
      "✓ Entrenamiento completo\n",
      "================================================================================\n",
      "  Tiempo: 12.73 minutos\n",
      "  Épocas: 15\n",
      "  Mejor val_loss: 34.5122\n"
     ]
    }
   ],
   "source": [
    "#----------------------Entrenamiento----------------------------------------------\n",
    "print(\"Entrenamiento.\")\n",
    "\n",
    "EPOCHS = 15\n",
    "\n",
    "print(f\"\\n   Configuración:\")\n",
    "print(f\"   Épocas: {EPOCHS}\")\n",
    "print(f\"   Batches/época: {BATCHES_PER_EPOCH_TRAIN}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "history = {'loss': [], 'mae': [], 'val_loss': [], 'val_mae': []}\n",
    "\n",
    "print(\"\\n  Iniciando el entrenamiento\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    print(f\"\\nÉpoca {epoch+1}/{EPOCHS}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    epoch_losses = []\n",
    "    epoch_maes = []\n",
    "    \n",
    "    # Entrenar\n",
    "    batch_count = 0\n",
    "    try:\n",
    "        for X_batch, y_batch in train_generator.generate_batches(seed=epoch):\n",
    "            metrics = model.train_on_batch(X_batch, y_batch, return_dict=True)\n",
    "            epoch_losses.append(metrics['loss'])\n",
    "            epoch_maes.append(metrics['mae'])\n",
    "            batch_count += 1\n",
    "            \n",
    "            if batch_count % 100 == 0:\n",
    "                print(f\"  Batch {batch_count}/{BATCHES_PER_EPOCH_TRAIN} - \"\n",
    "                      f\"loss: {np.mean(epoch_losses[-20:]):.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error en batch {batch_count}: {e}\")\n",
    "        print(f\"  Siguiente epoca\")\n",
    "        continue\n",
    "    \n",
    "    if not epoch_losses:\n",
    "        print(\"  No se completaron batches, saltando época\")\n",
    "        continue\n",
    "    \n",
    "    train_loss = np.mean(epoch_losses)\n",
    "    train_mae = np.mean(epoch_maes)\n",
    "    \n",
    "    # Validación\n",
    "    val_losses = []\n",
    "    val_maes = []\n",
    "    for X_val, y_val in test_generator.generate_batches(seed=epoch):\n",
    "        val_metrics = model.test_on_batch(X_val, y_val, return_dict=True)\n",
    "        val_losses.append(val_metrics['loss'])\n",
    "        val_maes.append(val_metrics['mae'])\n",
    "    \n",
    "    val_loss = np.mean(val_losses) if val_losses else train_loss\n",
    "    val_mae = np.mean(val_maes) if val_maes else train_mae\n",
    "    \n",
    "    history['loss'].append(train_loss)\n",
    "    history['mae'].append(train_mae)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_mae'].append(val_mae)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"\\n    Época {epoch+1}:\")\n",
    "    print(f\"     loss: {train_loss:.4f} - mae: {train_mae:.4f}\")\n",
    "    print(f\"     val_loss: {val_loss:.4f} - val_mae: {val_mae:.4f}\")\n",
    "    print(f\"     Tiempo: {epoch_time:.1f}s\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if epoch > 3 and val_loss > history['val_loss'][-2]:\n",
    "        patience = getattr(model, 'patience', 0) + 1\n",
    "        model.patience = patience\n",
    "        if patience >= 3:\n",
    "            print(f\"\\n   Early stopping\")\n",
    "            break\n",
    "    else:\n",
    "        model.patience = 0\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Entrenamiento completo\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Tiempo: {training_time/60:.2f} minutos\")\n",
    "print(f\"  Épocas: {len(history['loss'])}\")\n",
    "print(f\"  Mejor val_loss: {min(history['val_loss']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51e68950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluando modelo.\n",
      "Resultados\n",
      "\n",
      "  R²:   0.8888 (88.9%)\n",
      "  RMSE: $5.5257\n",
      "  MAE:  $2.6541\n",
      "  MAPE: 20.93%\n",
      "\n",
      "  Evaluado en 409,168 predicciones\n"
     ]
    }
   ],
   "source": [
    "#----------------------Evaluación-------------------------------------------------\n",
    "print(\"\\nEvaluando modelo.\")\n",
    "\n",
    "eval_generator = RobustRDDBatchGenerator(\n",
    "    test_rdd,\n",
    "    batch_size=4096,\n",
    "    num_batches_per_epoch=100\n",
    ")\n",
    "\n",
    "all_predictions = []\n",
    "all_actuals = []\n",
    "\n",
    "for X_test, y_test_batch in eval_generator.generate_batches(seed=99):\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    all_predictions.extend(y_pred.flatten().tolist())\n",
    "    all_actuals.extend(y_test_batch.tolist())\n",
    "\n",
    "y_test_eval = np.array(all_actuals)\n",
    "y_pred_eval = np.array(all_predictions)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "mse = mean_squared_error(y_test_eval, y_pred_eval)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_eval, y_pred_eval)\n",
    "r2 = r2_score(y_test_eval, y_pred_eval)\n",
    "mape = np.mean(np.abs((y_test_eval - y_pred_eval) / y_test_eval)) * 100\n",
    "\n",
    "print(\"Resultados\")\n",
    "print(f\"\\n  R²:   {r2:.4f} ({r2*100:.1f}%)\")\n",
    "print(f\"  RMSE: ${rmse:.4f}\")\n",
    "print(f\"  MAE:  ${mae:.4f}\")\n",
    "print(f\"  MAPE: {mape:.2f}%\")\n",
    "print(f\"\\n  Evaluado en {len(y_test_eval):,} predicciones\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
