{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bfc0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------Liberias---------------------------------\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PySpark\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\hadoop\" # ConfiguraciÃ³n de Spark/Hadoop\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import col, rand\n",
    "from pyspark.sql.types import DoubleType, StructType, StructField\n",
    "\n",
    "# Keras/TensorFlow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Elephas - Deep Learning sobre Spark\n",
    "from elephas.spark_model import SparkModel\n",
    "from elephas.utils.rdd_utils import to_simple_rdd\n",
    "#---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "spark_session",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------Crear SparkSession-----------------------------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RedNeuronal_Elephas\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "#---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "generate_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2024-01-01 00:57:55|  2024-01-01 01:17:43|              1|         1.72|         1|                 N|         186|          79|           2|       17.7|  1.0|    0.5|       0.0|         0.0|                  1.0|        22.7|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:03:00|  2024-01-01 00:09:36|              1|          1.8|         1|                 N|         140|         236|           1|       10.0|  3.5|    0.5|      3.75|         0.0|                  1.0|       18.75|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:17:06|  2024-01-01 00:35:01|              1|          4.7|         1|                 N|         236|          79|           1|       23.3|  3.5|    0.5|       3.0|         0.0|                  1.0|        31.3|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:36:38|  2024-01-01 00:44:56|              1|          1.4|         1|                 N|          79|         211|           1|       10.0|  3.5|    0.5|       2.0|         0.0|                  1.0|        17.0|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:46:51|  2024-01-01 00:52:57|              1|          0.8|         1|                 N|         211|         148|           1|        7.9|  3.5|    0.5|       3.2|         0.0|                  1.0|        16.1|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#----------------------Cargar datos-------------------------------------------------------\n",
    "DATA_PATH = \"C:/Users/PC/Documents/DocumentosGustavo/Github/Maestria/BigData/nyc-taxi-spark/data/yellow/2024/yellow_tripdata_2024-01.parquet\"\n",
    "df = spark.read.parquet(DATA_PATH)\n",
    "df.show(5) #Se utiliza show para vizualizar los datos, es como head en pandas, aqui solo vizualiso el parquet de enero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ee942a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Total de registros: 2964624\n",
      "  Columnas: 19\n",
      "\n",
      "Esquema del dataset:\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"  Total de registros: {df.count()}\")\n",
    "print(f\"  Columnas: {len(df.columns)}\")\n",
    "print(\"\\nEsquema del dataset:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "186c500c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se creocon exito el RDD con las columnas seleccionadas.\n",
      "  Total registros: 2,964,624\n",
      "\n",
      "Ejemplo de registro:\n",
      "(1.72, 1, datetime.datetime(2024, 1, 1, 0, 57, 55), 17.7)\n"
     ]
    }
   ],
   "source": [
    "# Seleccionar las columnas necesarias\n",
    "rdd_features = df.select(\n",
    "    \"trip_distance\",\n",
    "    \"passenger_count\", \n",
    "    \"tpep_pickup_datetime\",\n",
    "    \"fare_amount\"\n",
    ").rdd.map(lambda row: (\n",
    "    row.trip_distance,\n",
    "    row.passenger_count,\n",
    "    row.tpep_pickup_datetime,\n",
    "    row.fare_amount\n",
    "))\n",
    "\n",
    "print(f\" Se creocon exito el RDD con las columnas seleccionadas.\")\n",
    "print(f\"  Total registros: {rdd_features.count():,}\")\n",
    "print(f\"\\nEjemplo de registro:\")\n",
    "print(rdd_features.take(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6a73014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FunciÃ³n de feature engineering con escalado definida\n",
      "Aplicando escalado distribuido.\n",
      "\n",
      " Escalado completado\n",
      "  Registros originales: 2,964,624\n",
      "  Registros escalados: 2,722,784\n",
      "  Registros filtrados: 241,840\n",
      "\n",
      "Ejemplo de registro escalado:\n",
      "  Features (escaladas): ['-0.256', '-0.500', '-1.714', '-1.500']\n",
      "  Label (fare original): $17.70\n"
     ]
    }
   ],
   "source": [
    "#----------------------Feature Engineering con Escalado EstÃ¡ndar-----------------\n",
    "def extract_and_scale_features(row):\n",
    "    \"\"\"   Extrae features y aplica escalado (Z-score) para usar con ReLU.\n",
    "    Input: (trip_distance, passenger_count, datetime, fare_amount)\n",
    "    Output: (features_list, label_list) o None si datos invÃ¡lidos\n",
    "    \"\"\"\n",
    "    trip_distance, passenger_count, datetime, fare_amount = row\n",
    "    \n",
    "    # ========== FILTROS DE VALIDACIÃ“N ==========\n",
    "    # Filtrar datos invÃ¡lidos o atÃ­picos (menos restrictivo que antes)\n",
    "    if (trip_distance is None or trip_distance <= 0 or trip_distance >= 100 or\n",
    "        passenger_count is None or passenger_count <= 0 or passenger_count > 6 or\n",
    "        datetime is None or\n",
    "        fare_amount is None or fare_amount <= 0 or fare_amount >= 200):\n",
    "        return None\n",
    "    \n",
    "    # ========== EXTRACCIÃ“N DE FEATURES TEMPORALES ==========\n",
    "    hour_value = float(datetime.hour)                # 0-23\n",
    "    day_of_week = float(datetime.weekday() + 1)      # 1=Lunes, 7=Domingo\n",
    "    \n",
    "    # ========== ESCALADO PARA MEJOR CONVERGENCIA CON ReLU ==========\n",
    "    # Z-score aproximado basado en conocimiento del dominio\n",
    "    \n",
    "    # Feature 1: Distancia del viaje (media ~3, std ~5)\n",
    "    trip_distance_scaled = (trip_distance - 3.0) / 5.0\n",
    "    \n",
    "    # Feature 2: NÃºmero de pasajeros (media ~1.5, std ~1)\n",
    "    passenger_count_scaled = (passenger_count - 1.5) / 1.0\n",
    "    \n",
    "    # Feature 3: Hora del dÃ­a (media ~12, std ~7)\n",
    "    hour_scaled = (hour_value - 12.0) / 7.0\n",
    "    \n",
    "    # Feature 4: DÃ­a de la semana (media ~4, std ~2)\n",
    "    day_scaled = (day_of_week - 4.0) / 2.0\n",
    "    \n",
    "    # Target: Tarifa (media ~15, std ~10) - Sin escalar para salida ReLU\n",
    "    # IMPORTANTE: La salida ReLU puede predecir valores positivos ilimitados\n",
    "    fare_target = float(fare_amount)\n",
    "    \n",
    "    # ========== RETORNAR FORMATO COMPATIBLE CON ELEPHAS ==========\n",
    "    features = [\n",
    "        float(trip_distance_scaled),\n",
    "        float(passenger_count_scaled),\n",
    "        float(hour_scaled),\n",
    "        float(day_scaled)\n",
    "    ]\n",
    "    \n",
    "    label = fare_target  # Target sin escalar para ReLU\n",
    "    \n",
    "    return (features, label)\n",
    "\n",
    "print(\"FunciÃ³n de feature engineering con escalado definida\")\n",
    "\n",
    "# Aplicar escalado DISTRIBUIDO al RDD\n",
    "print(\"Aplicando escalado distribuido.\")\n",
    "\n",
    "rdd_scaled = rdd_features.map(\n",
    "    lambda row: extract_and_scale_features(row)\n",
    ").filter(lambda x: x is not None).cache()\n",
    "\n",
    "# Contar registros vÃ¡lidos\n",
    "total_scaled = rdd_scaled.count()\n",
    "\n",
    "print(f\"\\n Escalado completado\")\n",
    "print(f\"  Registros originales: {rdd_features.count():,}\")\n",
    "print(f\"  Registros escalados: {total_scaled:,}\")\n",
    "print(f\"  Registros filtrados: {rdd_features.count() - total_scaled:,}\")\n",
    "\n",
    "# Mostrar ejemplo de dato escalado\n",
    "print(\"\\nEjemplo de registro escalado:\")\n",
    "sample = rdd_scaled.take(1)[0]\n",
    "print(f\"  Features (escaladas): {[f'{x:.3f}' for x in sample[0]]}\")\n",
    "print(f\"  Label (fare original): ${sample[1]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72a8c35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ DivisiÃ³n de datos completada\n",
      "  Train: 2,178,428 registros (80.0%)\n",
      "  Test:  544,356 registros (20.0%)\n"
     ]
    }
   ],
   "source": [
    "#----------------------DivisiÃ³n Train/Test---------------------------------------\n",
    "# Dividiendo los datos en entrenamiento (80%) y prueba (20%)\n",
    "train_rdd, test_rdd = rdd_scaled.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Cachear para mejor rendimiento\n",
    "train_rdd = train_rdd.cache()\n",
    "test_rdd = test_rdd.cache()\n",
    "\n",
    "train_count = train_rdd.count()\n",
    "test_count = test_rdd.count()\n",
    "\n",
    "print(f\"\\nâœ“ DivisiÃ³n de datos completada\")\n",
    "print(f\"  Train: {train_count:,} registros ({train_count/total_scaled*100:.1f}%)\")\n",
    "print(f\"  Test:  {test_count:,} registros ({test_count/total_scaled*100:.1f}%)\")\n",
    "\n",
    "#---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52f99524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Modelo Keras creado con arquitectura ReLU\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ capa_oculta_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ capa_oculta_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ capa_salida (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ capa_oculta_1 (\u001b[38;5;33mDense\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              â”‚            \u001b[38;5;34m40\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ capa_oculta_2 (\u001b[38;5;33mDense\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              â”‚            \u001b[38;5;34m36\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ capa_salida (\u001b[38;5;33mDense\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚             \u001b[38;5;34m5\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">81</span> (324.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m81\u001b[0m (324.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">81</span> (324.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m81\u001b[0m (324.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#----------------------ConstrucciÃ³n del Modelo Keras----------------------------\n",
    "# Arquitectura:\n",
    "# - Capa 1: 8 neuronas, activaciÃ³n ReLU\n",
    "# - Capa 2: 4 neuronas, activaciÃ³n ReLU  \n",
    "# - Capa 3: 1 neurona, activaciÃ³n ReLU\n",
    "\n",
    "def create_keras_model():\n",
    "    model = Sequential([\n",
    "        Dense(8, activation='relu', input_shape=(4,), name='capa_oculta_1'),   # 4 features de entrada\n",
    "        Dense(4, activation='relu', name='capa_oculta_2'),\n",
    "        Dense(1, activation='relu', name='capa_salida')  # ReLU en salida para valores positivos\n",
    "    ])\n",
    "    \n",
    "    # Compilar el modelo\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',  # Error cuadrÃ¡tico medio para regresiÃ³n\n",
    "        metrics=['mae']  # Error absoluto medio como mÃ©trica adicional\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear modelo local\n",
    "keras_model = create_keras_model()\n",
    "print(\"\\n Modelo Keras creado con arquitectura ReLU\")\n",
    "print(keras_model.summary())\n",
    "\n",
    "#---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e596e574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparando datos para Elephas...\n",
      "âœ“ RDDs preparados para Elephas\n",
      "  Muestra train: [([-0.256, -0.5, -1.7142857142857142, -1.5], 17.7)]\n"
     ]
    }
   ],
   "source": [
    "#----------------------PreparaciÃ³n para Elephas (MÃ‰TODO CORREGIDO)---------------\n",
    "print(\"\\nPreparando datos para Elephas...\")\n",
    "\n",
    "# Crear el formato especÃ­fico que espera Elephas\n",
    "def prepare_for_elephas(row):\n",
    "    \"\"\"Prepara cada fila para Elephas\"\"\"\n",
    "    features, label = row\n",
    "    # Elephas espera: (features_array, label)\n",
    "    return (features.tolist() if hasattr(features, 'tolist') else features, float(label))\n",
    "\n",
    "# Aplicar transformaciÃ³n y cachear\n",
    "train_elephas_rdd = train_rdd.map(prepare_for_elephas).cache()\n",
    "test_elephas_rdd = test_rdd.map(prepare_for_elephas).cache()\n",
    "\n",
    "# Forzar evaluaciÃ³n\n",
    "train_elephas_rdd.count()\n",
    "test_elephas_rdd.count()\n",
    "\n",
    "print(\"âœ“ RDDs preparados para Elephas\")\n",
    "print(f\"  Muestra train: {train_elephas_rdd.take(1)}\")\n",
    "\n",
    "#---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb8095af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PASO 10: Configurando Elephas para entrenamiento distribuido\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ ConfiguraciÃ³n de entrenamiento distribuido:\n",
      "  â€¢ Batch size: 256\n",
      "  â€¢ Ã‰pocas: 10\n",
      "  â€¢ Workers: 4\n",
      "  â€¢ Registros train: 2,178,428\n",
      "  â€¢ Registros test: 544,356\n",
      "\n",
      "ğŸ”§ Creando SparkModel de Elephas...\n",
      "âœ“ SparkModel creado exitosamente\n",
      "  Modo: AsÃ­ncrono (mejor rendimiento)\n",
      "  SincronizaciÃ³n: Por Ã©poca\n"
     ]
    }
   ],
   "source": [
    "#----------------------PASO 10: Crear el SparkModel de Elephas------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PASO 10: Configurando Elephas para entrenamiento distribuido\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ConfiguraciÃ³n de Elephas\n",
    "from elephas.spark_model import SparkModel\n",
    "\n",
    "# ParÃ¡metros de entrenamiento distribuido\n",
    "BATCH_SIZE = 256        # TamaÃ±o de batch por worker\n",
    "EPOCHS = 10             # NÃºmero de Ã©pocas\n",
    "NUM_WORKERS = 4         # NÃºmero de workers (ajusta segÃºn tu mÃ¡quina)\n",
    "VERBOSE = 1             # Nivel de verbosidad\n",
    "\n",
    "print(f\"\\nğŸ“‹ ConfiguraciÃ³n de entrenamiento distribuido:\")\n",
    "print(f\"  â€¢ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  â€¢ Ã‰pocas: {EPOCHS}\")\n",
    "print(f\"  â€¢ Workers: {NUM_WORKERS}\")\n",
    "print(f\"  â€¢ Registros train: {train_count:,}\")\n",
    "print(f\"  â€¢ Registros test: {test_count:,}\")\n",
    "\n",
    "# Crear el modelo distribuido de Elephas\n",
    "print(\"\\nğŸ”§ Creando SparkModel de Elephas...\")\n",
    "\n",
    "spark_model = SparkModel(\n",
    "    model=keras_model,           # Tu modelo Keras\n",
    "    frequency='epoch',           # Frecuencia de sincronizaciÃ³n ('epoch' o 'batch')\n",
    "    mode='asynchronous',         # Modo asÃ­ncrono para mejor rendimiento\n",
    "    num_workers=NUM_WORKERS      # NÃºmero de workers para paralelizar\n",
    ")\n",
    "\n",
    "print(\"âœ“ SparkModel creado exitosamente\")\n",
    "print(f\"  Modo: AsÃ­ncrono (mejor rendimiento)\")\n",
    "print(f\"  SincronizaciÃ³n: Por Ã©poca\")\n",
    "\n",
    "#---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d91995f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PASO 11: Entrenamiento distribuido con Elephas\n",
      "================================================================================\n",
      "\n",
      "ğŸ¯ Iniciando entrenamiento distribuido...\n",
      "â±ï¸  Este proceso puede tomar varios minutos con ~2.2M registros\n",
      "\n",
      ">>> Fit model\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 24.0 failed 1 times, most recent failure: Lost task 1.0 in stage 24.0 (TID 123) (DESKTOP-OEGMP3C executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:678)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:663)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1034)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\r\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\r\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\r\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1528)\r\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1521)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1057)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2536)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1022)\r\n\t... 22 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\r\n\tat scala.collection.immutable.List.foreach(List.scala:323)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1017)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2496)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2517)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2536)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2561)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:205)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:678)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:663)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1034)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\r\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\r\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\r\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1528)\r\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1521)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1057)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2536)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1022)\r\n\t... 22 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     11\u001b[39m start_time = time.time()\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# ENTRENAR EL MODELO DISTRIBUIDO\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Elephas distribuye automÃ¡ticamente el entrenamiento entre workers\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mspark_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_elephas_rdd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mVERBOSE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 10% de train para validaciÃ³n\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m training_time = time.time() - start_time\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mâœ“ Entrenamiento completado en \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_time/\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m minutos\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\elephas\\spark_model.py:173\u001b[39m, in \u001b[36mSparkModel.fit\u001b[39m\u001b[34m(self, rdd, **kwargs)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_workers:\n\u001b[32m    171\u001b[39m     rdd = rdd.repartition(\u001b[38;5;28mself\u001b[39m.num_workers)\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\elephas\\spark_model.py:203\u001b[39m, in \u001b[36mSparkModel._fit\u001b[39m\u001b[34m(self, rdd, **kwargs)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    194\u001b[39m     worker = SparkWorker(\n\u001b[32m    195\u001b[39m         model_json,\n\u001b[32m    196\u001b[39m         parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    201\u001b[39m         custom,\n\u001b[32m    202\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     training_outcomes = \u001b[43mrdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     new_parameters = [w.copy() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m parameters.value]\n\u001b[32m    205\u001b[39m     number_of_sub_models = \u001b[38;5;28mlen\u001b[39m(training_outcomes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\core\\rdd.py:1700\u001b[39m, in \u001b[36mRDD.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1698\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m.context):\n\u001b[32m   1699\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1700\u001b[39m     sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonRDD\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jrdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1701\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m._jrdd_deserializer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 24.0 failed 1 times, most recent failure: Lost task 1.0 in stage 24.0 (TID 123) (DESKTOP-OEGMP3C executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:678)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:663)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1034)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\r\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\r\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\r\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1528)\r\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1521)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1057)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2536)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1022)\r\n\t... 22 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\r\n\tat scala.collection.immutable.List.foreach(List.scala:323)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1017)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2496)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2517)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2536)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2561)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:205)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:678)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:663)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1034)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\r\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\r\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\r\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1528)\r\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1521)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1057)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2536)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1022)\r\n\t... 22 more\r\n"
     ]
    }
   ],
   "source": [
    "#----------------------PASO 11: Entrenar el modelo distribuido-------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PASO 11: Entrenamiento distribuido con Elephas\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"\\nğŸ¯ Iniciando entrenamiento distribuido...\")\n",
    "print(\"â±ï¸  Este proceso puede tomar varios minutos con ~2.2M registros\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# ENTRENAR EL MODELO DISTRIBUIDO\n",
    "# Elephas distribuye automÃ¡ticamente el entrenamiento entre workers\n",
    "spark_model.fit(\n",
    "    train_elephas_rdd,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=VERBOSE,\n",
    "    validation_split=0.1  # 10% de train para validaciÃ³n\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâœ“ Entrenamiento completado en {training_time/60:.2f} minutos\")\n",
    "print(f\"  Tiempo por Ã©poca: ~{training_time/EPOCHS:.1f} segundos\")\n",
    "\n",
    "#---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0ef8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
