{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bfc0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------Liberias---------------------------------\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PySpark\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\hadoop\" # Configuración de Spark/Hadoop\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import col, rand\n",
    "from pyspark.sql.types import DoubleType, StructType, StructField\n",
    "\n",
    "# Keras/TensorFlow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Elephas - Deep Learning sobre Spark\n",
    "from elephas.spark_model import SparkModel\n",
    "from elephas.utils.rdd_utils import to_simple_rdd\n",
    "#---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "spark_session",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------Crear SparkSession-----------------------------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RedNeuronal_Elephas\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "#---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "generate_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2024-01-01 00:57:55|  2024-01-01 01:17:43|              1|         1.72|         1|                 N|         186|          79|           2|       17.7|  1.0|    0.5|       0.0|         0.0|                  1.0|        22.7|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:03:00|  2024-01-01 00:09:36|              1|          1.8|         1|                 N|         140|         236|           1|       10.0|  3.5|    0.5|      3.75|         0.0|                  1.0|       18.75|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:17:06|  2024-01-01 00:35:01|              1|          4.7|         1|                 N|         236|          79|           1|       23.3|  3.5|    0.5|       3.0|         0.0|                  1.0|        31.3|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:36:38|  2024-01-01 00:44:56|              1|          1.4|         1|                 N|          79|         211|           1|       10.0|  3.5|    0.5|       2.0|         0.0|                  1.0|        17.0|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:46:51|  2024-01-01 00:52:57|              1|          0.8|         1|                 N|         211|         148|           1|        7.9|  3.5|    0.5|       3.2|         0.0|                  1.0|        16.1|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#----------------------Cargar datos-------------------------------------------------------\n",
    "DATA_PATH = \"C:/Users/PC/Documents/DocumentosGustavo/Github/Maestria/BigData/nyc-taxi-spark/data/yellow/2024/yellow_tripdata_2024-01.parquet\"\n",
    "df = spark.read.parquet(DATA_PATH)\n",
    "df.show(5) #Se utiliza show para vizualizar los datos, es como head en pandas, aqui solo vizualiso el parquet de enero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ee942a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Total de registros: 2964624\n",
      "  Columnas: 19\n",
      "\n",
      "Esquema del dataset:\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"  Total de registros: {df.count()}\")\n",
    "print(f\"  Columnas: {len(df.columns)}\")\n",
    "print(\"\\nEsquema del dataset:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "186c500c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se creocon exito el RDD con las columnas seleccionadas.\n",
      "  Total registros: 2,964,624\n",
      "\n",
      "Ejemplo de registro:\n",
      "(1.72, 1, datetime.datetime(2024, 1, 1, 0, 57, 55), 17.7)\n"
     ]
    }
   ],
   "source": [
    "# Seleccionar las columnas necesarias\n",
    "rdd_features = df.select(\n",
    "    \"trip_distance\",\n",
    "    \"passenger_count\", \n",
    "    \"tpep_pickup_datetime\",\n",
    "    \"fare_amount\"\n",
    ").rdd.map(lambda row: (\n",
    "    row.trip_distance,\n",
    "    row.passenger_count,\n",
    "    row.tpep_pickup_datetime,\n",
    "    row.fare_amount\n",
    "))\n",
    "\n",
    "print(f\" Se creocon exito el RDD con las columnas seleccionadas.\")\n",
    "print(f\"  Total registros: {rdd_features.count():,}\")\n",
    "print(f\"\\nEjemplo de registro:\")\n",
    "print(rdd_features.take(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9fc708e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Función de feature engineering definida\n"
     ]
    }
   ],
   "source": [
    "def extract_and_normalize_features(row):\n",
    "    \"\"\"\n",
    "    Input: (trip_distance, passenger_count, datetime, fare_amount)\n",
    "    Output: (features_list, label_list) o None si datos inválidos\n",
    "    \n",
    "    Normalización a rango [0, 1] necesaria para activación sigmoid\n",
    "    \"\"\"\n",
    "    trip_distance, passenger_count, datetime, fare_amount = row\n",
    "    \n",
    "    # ========== FILTROS DE VALIDACIÓN ==========\n",
    "    # Filtrar datos inválidos o atípicos\n",
    "    if (trip_distance is None or trip_distance <= 0 or trip_distance >= 100 or\n",
    "        passenger_count is None or passenger_count <= 0 or passenger_count > 6 or\n",
    "        datetime is None or\n",
    "        fare_amount is None or fare_amount <= 0 or fare_amount >= 200):\n",
    "        return None\n",
    "    \n",
    "    # ========== EXTRACCIÓN DE FEATURES TEMPORALES ==========\n",
    "    hour_value = datetime.hour                # 0-23\n",
    "    day_of_week = datetime.weekday() + 1     # 1=Lunes, 7=Domingo\n",
    "    \n",
    "    # ========== NORMALIZACIÓN A [0, 1] ==========\n",
    "    # Necesario para función de activación sigmoid\n",
    "    \n",
    "    # Feature 1: Distancia del viaje\n",
    "    trip_distance_norm = min(trip_distance / 100.0, 1.0)  # Max: 100 millas\n",
    "    \n",
    "    # Feature 2: Número de pasajeros\n",
    "    passenger_count_norm = passenger_count / 6.0          # Max: 6 pasajeros\n",
    "    \n",
    "    # Feature 3: Hora del día\n",
    "    hour_norm = hour_value / 24.0                         # Max: 24 horas\n",
    "    \n",
    "    # Feature 4: Día de la semana\n",
    "    day_norm = day_of_week / 7.0                          # Max: 7 días\n",
    "    \n",
    "    # Target: Tarifa normalizada\n",
    "    # Min: $2, Max: $200 → Rango [0, 1]\n",
    "    fare_norm = (fare_amount - 2.0) / (200.0 - 2.0)\n",
    "    fare_norm = max(0.0, min(fare_norm, 1.0))  # Clamp a [0, 1]\n",
    "    \n",
    "    # ========== RETORNAR FORMATO COMPATIBLE CON ELEPHAS ==========\n",
    "    # Formato: ([features], label) donde label es float, NO lista\n",
    "    features = [\n",
    "        float(trip_distance_norm),\n",
    "        float(passenger_count_norm),\n",
    "        float(hour_norm),\n",
    "        float(day_norm)\n",
    "    ]\n",
    "    \n",
    "    label = float(fare_norm)  # Elephas espera un float, no lista\n",
    "    \n",
    "    return (features, label)\n",
    "\n",
    "print(\"✓ Función de feature engineering definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdcbbed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando normalización distribuida...\n",
      "\n",
      "✓ Normalización completada\n",
      "  Registros originales: 2,964,624\n",
      "  Registros normalizados: 2,722,784\n",
      "  Registros filtrados: 241,840\n",
      "\n",
      "Ejemplo de registro normalizado:\n",
      "  Features: [0.0172, 0.16666666666666666, 0.0, 0.14285714285714285]\n",
      "  Label (fare): 0.0793\n"
     ]
    }
   ],
   "source": [
    "# Aplicar normalización DISTRIBUIDA al RDD\n",
    "print(\"Aplicando normalización distribuida...\")\n",
    "\n",
    "rdd_normalized = rdd_features.map(\n",
    "    lambda row: extract_and_normalize_features(row)\n",
    ").filter(lambda x: x is not None).cache()\n",
    "\n",
    "# Contar registros válidos\n",
    "total_normalized = rdd_normalized.count()\n",
    "\n",
    "print(f\"\\n✓ Normalización completada\")\n",
    "print(f\"  Registros originales: {rdd_features.count():,}\")\n",
    "print(f\"  Registros normalizados: {total_normalized:,}\")\n",
    "print(f\"  Registros filtrados: {rdd_features.count() - total_normalized:,}\")\n",
    "\n",
    "# Mostrar ejemplo de dato normalizado\n",
    "print(\"\\nEjemplo de registro normalizado:\")\n",
    "sample = rdd_normalized.take(1)[0]\n",
    "print(f\"  Features: {sample[0]}\")\n",
    "print(f\"  Label (fare): {sample[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e191fab8",
   "metadata": {},
   "source": [
    "Variables del modeo:\n",
    "- trip_distance: Distancia del viaje (normalizada a [0,1])\n",
    "- passenger_count: Número de pasajeros (normalizada a [0,1])\n",
    "- hour: Hora del día 0-23 (normalizada a [0,1])\n",
    "- day_of_week: Día de la semana 1-7 (normalizada a [0,1])\n",
    "\n",
    "Variable predictoria: \n",
    "- fare_amount: Tarifa del viaje (normalizada a [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1e80149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función definida\n"
     ]
    }
   ],
   "source": [
    "def extract_and_normalize_features(row):\n",
    "    \"\"\"\n",
    "    Extrae y normaliza features de cada registro.\n",
    "    Input: (trip_distance, passenger_count, datetime, fare_amount)\n",
    "    Output: (features_list, label_list) o None si datos inválidos\n",
    "    \n",
    "    Normalización a rango [0, 1] necesaria igmoid\n",
    "    \"\"\"\n",
    "    trip_distance, passenger_count, datetime, fare_amount = row\n",
    "    \n",
    "    # Filtrar datos inválidos o atípicos\n",
    "    if (trip_distance is None or trip_distance <= 0 or trip_distance >= 100 or\n",
    "        passenger_count is None or passenger_count <= 0 or passenger_count > 6 or\n",
    "        datetime is None or\n",
    "        fare_amount is None or fare_amount <= 0 or fare_amount >= 200):\n",
    "        return None\n",
    "    \n",
    "    #Extraccion de datos temporales\n",
    "    hour_value = datetime.hour                # 0-23\n",
    "    day_of_week = datetime.weekday() + 1     # 1=Lunes, 7=Domingo\n",
    "    \n",
    "    # normalizacion de 0 a 1     \n",
    "    # Feature 1: Distancia del viaje\n",
    "    trip_distance_norm = min(trip_distance / 100.0, 1.0)  # Max: 100 millas\n",
    "    \n",
    "    # Feature 2: Número de pasajeros\n",
    "    passenger_count_norm = passenger_count / 6.0          # Max: 6 pasajeros\n",
    "    \n",
    "    # Feature 3: Hora del día\n",
    "    hour_norm = hour_value / 24.0                         # Max: 24 horas\n",
    "    \n",
    "    # Feature 4: Día de la semana\n",
    "    day_norm = day_of_week / 7.0                          # Max: 7 días\n",
    "    \n",
    "    # Target: Tarifa normalizada\n",
    "    # Min: $2, Max: $200 a  Rango [0, 1]\n",
    "    fare_norm = (fare_amount - 2.0) / (200.0 - 2.0)\n",
    "    fare_norm = max(0.0, min(fare_norm, 1.0))  # Clamp a [0, 1]\n",
    "    \n",
    "    # Retornar formato compatible con elephas\n",
    "    # Formato: ([features], label) donde label es float, no lista\n",
    "    features = [\n",
    "        float(trip_distance_norm),\n",
    "        float(passenger_count_norm),\n",
    "        float(hour_norm),\n",
    "        float(day_norm)\n",
    "    ]\n",
    "    \n",
    "    label = float(fare_norm)  # Elephas espera un float, no lista\n",
    "    \n",
    "    return (features, label)\n",
    "\n",
    "print(\"Función definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167f8492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar normalización distribuida al rdd\n",
    "rdd_normalized = rdd_features.map(\n",
    "    lambda row: extract_and_normalize_features(row)\n",
    ").filter(lambda x: x is not None).cache()\n",
    "\n",
    "# Contar registros válidos\n",
    "total_normalized = rdd_normalized.count()\n",
    "\n",
    "print(f\"  Registros originales: {rdd_features.count():,}\")\n",
    "print(f\"  Registros normalizados: {total_normalized:,}\")\n",
    "print(f\"  Registros filtrados: {rdd_features.count() - total_normalized:,}\")\n",
    "\n",
    "# Mostrar ejemplo de dato normalizado\n",
    "print(\"\\n Ejemplo de registro normalizado:\")\n",
    "sample = rdd_normalized.take(1)[0]\n",
    "print(f\"  Features: {sample[0]}\")\n",
    "print(f\"  Label (fare): {sample[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169bde61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train: 2,178,428 registros (80.0%)\n",
      "  Test:  544,356 registros (20.0%)\n"
     ]
    }
   ],
   "source": [
    "#Dividiendo los datos en entrenamiento (80%) y prueba (20%) usando randomSplit en RDD\n",
    "train_rdd, test_rdd = rdd_normalized.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Cachear para mejor rendimiento\n",
    "train_rdd = train_rdd.cache()\n",
    "test_rdd = test_rdd.cache()\n",
    "\n",
    "train_count = train_rdd.count()\n",
    "test_count = test_rdd.count()\n",
    "\n",
    "print(f\"  Train: {train_count:,} registros ({train_count/total_normalized*100:.1f}%)\")\n",
    "print(f\"  Test:  {test_count:,} registros ({test_count/total_normalized*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a25e7e",
   "metadata": {},
   "source": [
    "Arquitectura de la red neuronal: \n",
    "- Capa oculta: 4 neuronas con activación sigmoide\n",
    "- Capa de salida: 1 neurona con activación sigmoide\n",
    "- Función de perdida: MAE\n",
    "- Optimizador: Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605c6527",
   "metadata": {},
   "source": [
    "Entrenamiento del modelo con Elephas (distribuido).\n",
    "- Mini-batch: 500\n",
    "- Epocas: 5\n",
    "- Modo: entrenamiento distibuido synchronous\n",
    "- workers: 2 (palalelismo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
