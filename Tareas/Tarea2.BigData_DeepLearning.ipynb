{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------- Librerias ----------------------------------------------\n",
    "#Librerias tipicas para el análisis de datos\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Librerias para la implementación de pyspark\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\hadoop\"\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Libreraias para la implementación de keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "spark_config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sessión de Spark creada\n",
      "  Network timeout: 800s\n",
      "  Worker timeout: 600s\n"
     ]
    }
   ],
   "source": [
    "#-----------------------SparkSession-----------------------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC_Taxi_Spark_DeepLearning\") \\\n",
    "    .master(\"local[8]\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .config(\"spark.python.worker.timeout\", \"600s\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"16\") \\\n",
    "    .config(\"spark.default.parallelism\", \"16\") \\\n",
    "    .config(\"spark.rdd.compress\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# appName: Nombre de la aplicación de spark \n",
    "# master, local[8] ejecuta localmente con 8 threads (uno por core)\n",
    "# driver.memory 12g: memoria ram para el driver, mi computadora tiene 16gb de ram\n",
    "# executor.memory 12: memoria ram para los workers \n",
    "# spark.driver.maxResultSize: Máximo tamaño de resultados que collect() puede traer\n",
    "# .config(\"spark.network.timeout\", \"800s\") Timeout para comunicación entre nodos, por default 120s (2 minutos), mi ajuste  800s (13 minutos) para evitar timeouts con datos grandes.\n",
    "# .config(\"spark.executor.heartbeatInterval\", \"60s\"): Frecuencia con que ejecutores reportan al driver que siguen vivos. Default: 10s. Mi ajuste : 60s reduce overhead de comunicación.\n",
    "# .config(\"spark.python.worker.timeout\", \"600s\"): Timeout específico para workers de Python. Default: 120s. MI ajuste: 600s (10 minutos) para operaciones Python lentas.\n",
    "# .config(\"spark.default.parallelism\", \"16\"): Paralelismo por defecto para operaciones RDD. Mi ajuste: 16 particiones = 2 por core. \n",
    "# config(\"spark.rdd.compress\", \"true\"): Comprimir RDDs serializados en memoria. Ventaja: Usa menos RAM (importante para cache). Costo: CPU extra para comprimir/descomprimir. \n",
    "# .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\"): Protocolo de serialización de objetos. Default: Java serialization (lento). Kryo: 10x más rápido y usa menos memoria.\n",
    "\n",
    "print(\" Sessión de Spark creada\")\n",
    "print(\"  Network timeout: 800s\")\n",
    "print(\"  Worker timeout: 600s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "load",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Cargando dataset\n",
      " Numero de registros: 2,964,624\n",
      " Columnas: 19\n"
     ]
    }
   ],
   "source": [
    "#----------------------Cargar datos-----------------------------------------------\n",
    "DATA_PATH = \"C:/Users/PC/Documents/DocumentosGustavo/Github/Maestria/BigData/nyc-taxi-spark/data/yellow/2024/yellow_tripdata_2024-01.parquet\"\n",
    "\n",
    "print(\"\\n Cargando dataset\")\n",
    "df = spark.read.parquet(DATA_PATH)\n",
    "print(f\" Numero de registros: {df.count():,}\")\n",
    "print(f\" Columnas: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0540270f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Esquema del dataset:\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEsquema del dataset:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b65e7316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeros 5 registros\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2024-01-01 00:57:55|  2024-01-01 01:17:43|              1|         1.72|         1|                 N|         186|          79|           2|       17.7|  1.0|    0.5|       0.0|         0.0|                  1.0|        22.7|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:03:00|  2024-01-01 00:09:36|              1|          1.8|         1|                 N|         140|         236|           1|       10.0|  3.5|    0.5|      3.75|         0.0|                  1.0|       18.75|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:17:06|  2024-01-01 00:35:01|              1|          4.7|         1|                 N|         236|          79|           1|       23.3|  3.5|    0.5|       3.0|         0.0|                  1.0|        31.3|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:36:38|  2024-01-01 00:44:56|              1|          1.4|         1|                 N|          79|         211|           1|       10.0|  3.5|    0.5|       2.0|         0.0|                  1.0|        17.0|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:46:51|  2024-01-01 00:52:57|              1|          0.8|         1|                 N|         211|         148|           1|        7.9|  3.5|    0.5|       3.2|         0.0|                  1.0|        16.1|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"Primeros 5 registros\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1339ab52",
   "metadata": {},
   "source": [
    "Para la implementación de esta tarea se elegiran las variables: \n",
    "- trip_distance\n",
    "- passenger_count\n",
    "- tpep_pickup_datetime\n",
    "- fare_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "feature_eng",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando features\n",
      "Registros despues de la limpieza: 2,722,784 registros. \n",
      " Completado en 50.5s.\n"
     ]
    }
   ],
   "source": [
    "#----------------------Feature Engineering----------------------------------------\n",
    "def extract_and_scale_features(row):\n",
    "    trip_distance, passenger_count, datetime, fare_amount = row\n",
    "    \n",
    "    if (trip_distance is None or trip_distance <= 0 or trip_distance >= 100 or\n",
    "        passenger_count is None or passenger_count <= 0 or passenger_count > 6 or\n",
    "        datetime is None or\n",
    "        fare_amount is None or fare_amount <= 0 or fare_amount >= 200):\n",
    "        return None\n",
    "    \"\"\"\n",
    "    trip_distance <= 0: Viajes inválidos (errores de sensor).\n",
    "    trip_distance >= 100: Outliers extremos (probablemente errores).\n",
    "    passenger_count <= 0 or > 6: NYC taxis tienen máximo 5 pasajeros + 1 niño.\n",
    "    fare_amount <= 0 or >= 200: Errores de medición o fraudes.\n",
    "\n",
    "    \"\"\"\n",
    "    hour_value = float(datetime.hour) #hora del día: 0-23\n",
    "    day_of_week = float(datetime.weekday() + 1) #dia de la semana: L-D\n",
    "    \n",
    "    features = [\n",
    "        float((trip_distance - 3.0) / 5.0),\n",
    "        float((passenger_count - 1.5) / 1.0),\n",
    "        float((hour_value - 12.0) / 7.0),\n",
    "        float((day_of_week - 4.0) / 2.0)\n",
    "    ]\n",
    "    \"\"\"\n",
    "    Normalización:  z = (x - μ) / σ, esto por que \n",
    "    Convergencia: Gradientes similares en todas las features.\n",
    "    Velocidad: Adam converge más rápido con datos escalados.\n",
    "    Estabilidad numérica: Evita overflow/underflow.\n",
    "    ReLU: Funciona mejor con datos centrados en 0.\n",
    "\n",
    "    \"\"\"\n",
    "    return (features, float(fare_amount)) #Estructura: Tupla (lista_features, label). Compatible con RDD map-reduce y Keras.\n",
    "\n",
    "print(\"\\nProcesando features\")\n",
    "start = time.time()\n",
    "\n",
    "rdd_features = df.select( #convierte a rdd\n",
    "    \"trip_distance\", \"passenger_count\", \"tpep_pickup_datetime\", \"fare_amount\"\n",
    ").rdd.map(lambda row: (\n",
    "    row.trip_distance, row.passenger_count, row.tpep_pickup_datetime, row.fare_amount # Transformación 1-a-1. Ejecución: Lazy (no se ejecuta hasta una acción).\n",
    ")) #Variables que se eligieron para realizar el proyecto\n",
    "\n",
    "rdd_scaled = rdd_features \\\n",
    "    .map(extract_and_scale_features) \\\n",
    "    .filter(lambda x: x is not None) \\\n",
    "    .repartition(16) \\\n",
    "    .cache()\n",
    "# lambda x:x is not None:  Elimina registros None (inválidos)\n",
    "# .repartition(16): Redistribuir datos en 16 particiones balanceadas\n",
    "total_scaled = rdd_scaled.count()\n",
    "print(f\"Registros despues de la limpieza: {total_scaled:,} registros. \\n Completado en {time.time()-start:.1f}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "split",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train: 2,177,985 registros\n",
      " Test: 544,799 registros\n"
     ]
    }
   ],
   "source": [
    "#----------------------División Train/Test----------------------------------------\n",
    "train_rdd, test_rdd = rdd_scaled.randomSplit([0.8, 0.2], seed=42) #80% entrenamiento y 20% prueba \n",
    "\n",
    "from pyspark import StorageLevel\n",
    "train_rdd = train_rdd.repartition(16).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "test_rdd = test_rdd.repartition(8).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "#StorageLevel.MEMORY_AND_DISK: primer intenta usar memoria, sino cabe utiliza disco\n",
    "\n",
    "train_count = train_rdd.count()\n",
    "test_count = test_rdd.count()\n",
    "\n",
    "print(f\"\\n Train: {train_count:,} registros\")\n",
    "print(f\" Test: {test_count:,} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155992e7",
   "metadata": {},
   "source": [
    "Arquitectura de la red neuronal: \n",
    "- capa oculta 1: 64 neuronas, función de activación relu\n",
    "- capa oculta 2: 32 neuronas, función de activación relu\n",
    "- capa oculta 3: 16 neuronas, función de activación relu\n",
    "- capa oculta 4:  8 neuronas, función de activación relu\n",
    "- capa de salida: 1 neurona , función de activación lineal\n",
    "\n",
    "- Optimizador: Adam\n",
    "- Función de costo: MSE\n",
    "- Metricas adicionales: MAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Modelo creado\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m9\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,457</span> (13.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,457\u001b[0m (13.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,265</span> (12.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,265\u001b[0m (12.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#----------------------Modelo-----------------------------------------------------\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(4,)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(0.001), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "print(\"\\n Modelo creado\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "generator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch size: 4096\n",
      "  Batches/época train: 400\n",
      "  Samples/época: 1,638,400\n"
     ]
    }
   ],
   "source": [
    "#----------------------Generador de batches--------------------------------\n",
    "class RobustRDDBatchGenerator: #Generador que usa toLocalIterator().\n",
    "    \n",
    "    def __init__(self, rdd, batch_size=4096, num_batches_per_epoch=None):\n",
    "        self.rdd = rdd\n",
    "        self.batch_size = batch_size\n",
    "        self.total_samples = rdd.count()\n",
    "        \n",
    "        if num_batches_per_epoch:\n",
    "            self.num_batches = num_batches_per_epoch\n",
    "        else:\n",
    "            self.num_batches = max(1, self.total_samples // batch_size)\n",
    "    \n",
    "    def generate_batches(self, seed=42):\n",
    "        \"\"\"\n",
    "        Genera batches usando toLocalIterator.\n",
    "        \n",
    "        toLocalIterator:\n",
    "        - Itera sobre RDD SIN collect masivo\n",
    "        - No causa timeout\n",
    "        - Procesa partición por partición\n",
    "        - 100% RDD distribuido\n",
    "        \"\"\"\n",
    "        # Sample del RDD\n",
    "        fraction = min(1.0, (self.batch_size * self.num_batches) / self.total_samples)\n",
    "        sampled_rdd = self.rdd.sample(False, fraction, seed=seed)\n",
    "        \n",
    "        # Usar toLocalIterator\n",
    "        batch_data = []\n",
    "        batch_count = 0\n",
    "        \n",
    "        \n",
    "        for item in sampled_rdd.toLocalIterator():\n",
    "            batch_data.append(item)\n",
    "            \n",
    "            # Cuando el batch está lleno, yield\n",
    "            if len(batch_data) >= self.batch_size:\n",
    "                X_batch = np.array([x[0] for x in batch_data], dtype=np.float32)\n",
    "                y_batch = np.array([x[1] for x in batch_data], dtype=np.float32)\n",
    "                \n",
    "                yield X_batch, y_batch\n",
    "                \n",
    "                batch_data = []\n",
    "                batch_count += 1\n",
    "                \n",
    "                # Limitar número de batches\n",
    "                if batch_count >= self.num_batches:\n",
    "                    break\n",
    "        \n",
    "        # Último batch parcial\n",
    "        if batch_data and batch_count < self.num_batches:\n",
    "            X_batch = np.array([x[0] for x in batch_data], dtype=np.float32)\n",
    "            y_batch = np.array([x[1] for x in batch_data], dtype=np.float32)\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "# Configuración\n",
    "BATCH_SIZE = 4096  \n",
    "BATCHES_PER_EPOCH_TRAIN = 400\n",
    "BATCHES_PER_EPOCH_VAL = 20\n",
    "\n",
    "train_generator = RobustRDDBatchGenerator(\n",
    "    train_rdd, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_batches_per_epoch=BATCHES_PER_EPOCH_TRAIN\n",
    ")\n",
    "\n",
    "test_generator = RobustRDDBatchGenerator(\n",
    "    test_rdd,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_batches_per_epoch=BATCHES_PER_EPOCH_VAL\n",
    ")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Batches/época train: {BATCHES_PER_EPOCH_TRAIN}\")\n",
    "print(f\"  Samples/época: {BATCH_SIZE * BATCHES_PER_EPOCH_TRAIN:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento.\n",
      "\n",
      "   Configuración:\n",
      "   Épocas: 15\n",
      "   Batches/época: 400\n",
      "   Batch size: 4096\n",
      "\n",
      "  Iniciando el entrenamiento\n",
      "\n",
      "\n",
      "Época 1/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 110.4774\n",
      "  Batch 200/400 - loss: 96.9188\n",
      "  Batch 300/400 - loss: 87.3430\n",
      "  Batch 400/400 - loss: 80.2727\n",
      "\n",
      "    Época 1:\n",
      "     loss: 98.4719 - mae: 5.7322\n",
      "     val_loss: 78.9331 - val_mae: 4.9447\n",
      "     Tiempo: 43.5s\n",
      "\n",
      "Época 2/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 73.5887\n",
      "  Batch 200/400 - loss: 69.4125\n",
      "  Batch 300/400 - loss: 65.9079\n",
      "  Batch 400/400 - loss: 63.0317\n",
      "\n",
      "    Época 2:\n",
      "     loss: 69.5014 - mae: 4.5533\n",
      "     val_loss: 62.4445 - val_mae: 4.2577\n",
      "     Tiempo: 42.9s\n",
      "\n",
      "Época 3/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 59.9962\n",
      "  Batch 200/400 - loss: 57.9194\n",
      "  Batch 300/400 - loss: 56.0852\n",
      "  Batch 400/400 - loss: 54.4739\n",
      "\n",
      "    Época 3:\n",
      "     loss: 57.8943 - mae: 4.0648\n",
      "     val_loss: 54.1179 - val_mae: 3.9033\n",
      "     Tiempo: 55.1s\n",
      "\n",
      "Época 4/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 52.6738\n",
      "  Batch 200/400 - loss: 51.4315\n",
      "  Batch 300/400 - loss: 50.2634\n",
      "  Batch 400/400 - loss: 49.2276\n",
      "\n",
      "    Época 4:\n",
      "     loss: 51.3765 - mae: 3.7820\n",
      "     val_loss: 49.0002 - val_mae: 3.6761\n",
      "     Tiempo: 59.0s\n",
      "\n",
      "Época 5/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 48.0415\n",
      "  Batch 200/400 - loss: 47.1929\n",
      "  Batch 300/400 - loss: 46.3998\n",
      "  Batch 400/400 - loss: 45.6731\n",
      "\n",
      "    Época 5:\n",
      "     loss: 47.1530 - mae: 3.5914\n",
      "     val_loss: 45.5121 - val_mae: 3.5154\n",
      "     Tiempo: 49.5s\n",
      "\n",
      "Época 6/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 44.8295\n",
      "  Batch 200/400 - loss: 44.2086\n",
      "  Batch 300/400 - loss: 43.6188\n",
      "  Batch 400/400 - loss: 43.0804\n",
      "\n",
      "    Época 6:\n",
      "     loss: 44.1734 - mae: 3.4522\n",
      "     val_loss: 42.9568 - val_mae: 3.3941\n",
      "     Tiempo: 53.4s\n",
      "\n",
      "Época 7/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 42.4467\n",
      "  Batch 200/400 - loss: 41.9740\n",
      "  Batch 300/400 - loss: 41.5142\n",
      "  Batch 400/400 - loss: 41.0723\n",
      "\n",
      "    Época 7:\n",
      "     loss: 41.9373 - mae: 3.3440\n",
      "     val_loss: 40.9763 - val_mae: 3.2971\n",
      "     Tiempo: 56.4s\n",
      "\n",
      "Época 8/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 40.5771\n",
      "  Batch 200/400 - loss: 40.1924\n",
      "  Batch 300/400 - loss: 39.8178\n",
      "  Batch 400/400 - loss: 39.4696\n",
      "\n",
      "    Época 8:\n",
      "     loss: 40.1636 - mae: 3.2563\n",
      "     val_loss: 39.3970 - val_mae: 3.2179\n",
      "     Tiempo: 56.2s\n",
      "\n",
      "Época 9/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 39.0625\n",
      "  Batch 200/400 - loss: 38.7409\n",
      "  Batch 300/400 - loss: 38.4282\n",
      "  Batch 400/400 - loss: 38.1275\n",
      "\n",
      "    Época 9:\n",
      "     loss: 38.7165 - mae: 3.1836\n",
      "     val_loss: 38.0667 - val_mae: 3.1510\n",
      "     Tiempo: 54.7s\n",
      "\n",
      "Época 10/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 37.7872\n",
      "  Batch 200/400 - loss: 37.5237\n",
      "  Batch 300/400 - loss: 37.2638\n",
      "  Batch 400/400 - loss: 37.0174\n",
      "\n",
      "    Época 10:\n",
      "     loss: 37.5013 - mae: 3.1219\n",
      "     val_loss: 36.9693 - val_mae: 3.0944\n",
      "     Tiempo: 54.7s\n",
      "\n",
      "Época 11/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 36.7343\n",
      "  Batch 200/400 - loss: 36.5077\n",
      "  Batch 300/400 - loss: 36.2840\n",
      "  Batch 400/400 - loss: 36.0754\n",
      "\n",
      "    Época 11:\n",
      "     loss: 36.4895 - mae: 3.0694\n",
      "     val_loss: 36.0363 - val_mae: 3.0456\n",
      "     Tiempo: 55.2s\n",
      "\n",
      "Época 12/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 35.8371\n",
      "  Batch 200/400 - loss: 35.6487\n",
      "  Batch 300/400 - loss: 35.4556\n",
      "  Batch 400/400 - loss: 35.2756\n",
      "\n",
      "    Época 12:\n",
      "     loss: 35.6310 - mae: 3.0238\n",
      "     val_loss: 35.2452 - val_mae: 3.0030\n",
      "     Tiempo: 56.5s\n",
      "\n",
      "Época 13/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 35.0694\n",
      "  Batch 200/400 - loss: 34.9096\n",
      "  Batch 300/400 - loss: 34.7417\n",
      "  Batch 400/400 - loss: 34.5808\n",
      "\n",
      "    Época 13:\n",
      "     loss: 34.8921 - mae: 2.9839\n",
      "     val_loss: 34.5556 - val_mae: 2.9656\n",
      "     Tiempo: 59.7s\n",
      "\n",
      "Época 14/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 34.4070\n",
      "  Batch 200/400 - loss: 34.2633\n",
      "  Batch 300/400 - loss: 34.1118\n",
      "  Batch 400/400 - loss: 33.9671\n",
      "\n",
      "    Época 14:\n",
      "     loss: 34.2466 - mae: 2.9488\n",
      "     val_loss: 33.9516 - val_mae: 2.9327\n",
      "     Tiempo: 62.3s\n",
      "\n",
      "Época 15/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 33.8277\n",
      "  Batch 200/400 - loss: 33.6921\n",
      "  Batch 300/400 - loss: 33.5559\n",
      "  Batch 400/400 - loss: 33.4264\n",
      "\n",
      "    Época 15:\n",
      "     loss: 33.6791 - mae: 2.9177\n",
      "     val_loss: 33.4135 - val_mae: 2.9033\n",
      "     Tiempo: 56.0s\n",
      "\n",
      "================================================================================\n",
      "✓ Entrenamiento completo\n",
      "================================================================================\n",
      "  Tiempo: 13.58 minutos\n",
      "  Épocas: 15\n",
      "  Mejor val_loss: 33.4135\n"
     ]
    }
   ],
   "source": [
    "#----------------------Entrenamiento----------------------------------------------\n",
    "print(\"Entrenamiento.\")\n",
    "\n",
    "EPOCHS = 15\n",
    "\n",
    "print(f\"\\n   Configuración:\")\n",
    "print(f\"   Épocas: {EPOCHS}\")\n",
    "print(f\"   Batches/época: {BATCHES_PER_EPOCH_TRAIN}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "history = {'loss': [], 'mae': [], 'val_loss': [], 'val_mae': []}\n",
    "\n",
    "print(\"\\n  Iniciando el entrenamiento\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    print(f\"\\nÉpoca {epoch+1}/{EPOCHS}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    epoch_losses = []\n",
    "    epoch_maes = []\n",
    "    \n",
    "    # Entrenar\n",
    "    batch_count = 0\n",
    "    try:\n",
    "        for X_batch, y_batch in train_generator.generate_batches(seed=epoch):\n",
    "            metrics = model.train_on_batch(X_batch, y_batch, return_dict=True)\n",
    "            epoch_losses.append(metrics['loss'])\n",
    "            epoch_maes.append(metrics['mae'])\n",
    "            batch_count += 1\n",
    "            \n",
    "            if batch_count % 100 == 0:\n",
    "                print(f\"  Batch {batch_count}/{BATCHES_PER_EPOCH_TRAIN} - \"\n",
    "                      f\"loss: {np.mean(epoch_losses[-20:]):.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error en batch {batch_count}: {e}\")\n",
    "        print(f\"  Siguiente epoca\")\n",
    "        continue\n",
    "    \n",
    "    if not epoch_losses:\n",
    "        print(\"  No se completaron batches, saltando época\")\n",
    "        continue\n",
    "    \n",
    "    train_loss = np.mean(epoch_losses)\n",
    "    train_mae = np.mean(epoch_maes)\n",
    "    \n",
    "    # Validación\n",
    "    val_losses = []\n",
    "    val_maes = []\n",
    "    for X_val, y_val in test_generator.generate_batches(seed=epoch):\n",
    "        val_metrics = model.test_on_batch(X_val, y_val, return_dict=True)\n",
    "        val_losses.append(val_metrics['loss'])\n",
    "        val_maes.append(val_metrics['mae'])\n",
    "    \n",
    "    val_loss = np.mean(val_losses) if val_losses else train_loss\n",
    "    val_mae = np.mean(val_maes) if val_maes else train_mae\n",
    "    \n",
    "    history['loss'].append(train_loss)\n",
    "    history['mae'].append(train_mae)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_mae'].append(val_mae)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"\\n    Época {epoch+1}:\")\n",
    "    print(f\"     loss: {train_loss:.4f} - mae: {train_mae:.4f}\")\n",
    "    print(f\"     val_loss: {val_loss:.4f} - val_mae: {val_mae:.4f}\")\n",
    "    print(f\"     Tiempo: {epoch_time:.1f}s\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if epoch > 3 and val_loss > history['val_loss'][-2]:\n",
    "        patience = getattr(model, 'patience', 0) + 1\n",
    "        model.patience = patience\n",
    "        if patience >= 3:\n",
    "            print(f\"\\n   Early stopping\")\n",
    "            break\n",
    "    else:\n",
    "        model.patience = 0\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Entrenamiento completo\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Tiempo: {training_time/60:.2f} minutos\")\n",
    "print(f\"  Épocas: {len(history['loss'])}\")\n",
    "print(f\"  Mejor val_loss: {min(history['val_loss']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "evaluate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluando modelo...\n",
      "Resultados\n",
      "\n",
      "  R²:   0.8837 (88.4%)\n",
      "  RMSE: $5.6731\n",
      "  MAE:  $2.6988\n",
      "  MAPE: 20.65%\n",
      "\n",
      "  Evaluado en 409,208 predicciones\n"
     ]
    }
   ],
   "source": [
    "#----------------------Evaluación-------------------------------------------------\n",
    "print(\"\\nEvaluando modelo...\")\n",
    "\n",
    "eval_generator = RobustRDDBatchGenerator(\n",
    "    test_rdd,\n",
    "    batch_size=4096,\n",
    "    num_batches_per_epoch=100\n",
    ")\n",
    "\n",
    "all_predictions = []\n",
    "all_actuals = []\n",
    "\n",
    "for X_test, y_test_batch in eval_generator.generate_batches(seed=99):\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    all_predictions.extend(y_pred.flatten().tolist())\n",
    "    all_actuals.extend(y_test_batch.tolist())\n",
    "\n",
    "y_test_eval = np.array(all_actuals)\n",
    "y_pred_eval = np.array(all_predictions)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "mse = mean_squared_error(y_test_eval, y_pred_eval)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_eval, y_pred_eval)\n",
    "r2 = r2_score(y_test_eval, y_pred_eval)\n",
    "mape = np.mean(np.abs((y_test_eval - y_pred_eval) / y_test_eval)) * 100\n",
    "\n",
    "print(\"Resultados\")\n",
    "print(f\"\\n  R²:   {r2:.4f} ({r2*100:.1f}%)\")\n",
    "print(f\"  RMSE: ${rmse:.4f}\")\n",
    "print(f\"  MAE:  ${mae:.4f}\")\n",
    "print(f\"  MAPE: {mape:.2f}%\")\n",
    "print(f\"\\n  Evaluado en {len(y_test_eval):,} predicciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------Guardar----------------------------------------------------\n",
    "#os.makedirs(\"modelos\", exist_ok=True)\n",
    "#model_path = f\"modelos/taxi_100RDD_ROBUSTO_{datetime.now().strftime('%Y%m%d_%H%M%S')}.h5\"\n",
    "#model.save(model_path)\n",
    "#print(f\"\\n✓ Modelo guardado: {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
