{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# üöÄ Tarea 2: Red Neuronal Big Data - OPTIMIZADO PARA VELOCIDAD\n",
    "## Dataset: NYC Taxi Enero 2024\n",
    "### Configuraci√≥n: 16GB RAM + 8 Cores = Entrenamiento Ultra-R√°pido\n",
    "\n",
    "**Optimizaciones:**\n",
    "- Aprovecha 16GB RAM disponible\n",
    "- Paralelizaci√≥n en 8 cores\n",
    "- Conversi√≥n eficiente de RDD (no batch-by-batch lento)\n",
    "- Cache agresivo\n",
    "- Batch size √≥ptimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------Librer√≠as---------------------------------\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PySpark\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\hadoop\"\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Keras/TensorFlow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Utilidades\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úì Librer√≠as importadas\")\n",
    "#---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spark_session",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------SparkSession OPTIMIZADO para 16GB + 8 Cores--------------------\n",
    "print(\"Configurando Spark para m√°ximo rendimiento...\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RedNeuronal_OPTIMIZADO\") \\\n",
    "    .master(\"local[8]\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"16\") \\\n",
    "    .config(\"spark.default.parallelism\", \"16\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"\\n‚úì Spark configurado para m√°ximo rendimiento\")\n",
    "print(f\"  Cores: 8 (todos disponibles)\")\n",
    "print(f\"  RAM: 12GB (de 16GB disponibles)\")\n",
    "print(f\"  Paralelismo: 16 particiones\")\n",
    "#---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------Cargar datos-----------------------------------------------\n",
    "DATA_PATH = \"C:/Users/PC/Documents/DocumentosGustavo/Github/Maestria/BigData/nyc-taxi-spark/data/yellow/2024/yellow_tripdata_2024-01.parquet\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CARGANDO DATASET NYC TAXI\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df = spark.read.parquet(DATA_PATH)\n",
    "\n",
    "print(f\"\\n‚úì Dataset cargado: {df.count():,} registros\")\n",
    "df.show(5)\n",
    "#---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------Feature Engineering DISTRIBUIDO---------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PASO 1: FEATURE ENGINEERING DISTRIBUIDO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def extract_and_scale_features(row):\n",
    "    \"\"\"\n",
    "    Extrae y escala features de forma distribuida.\n",
    "    \"\"\"\n",
    "    trip_distance, passenger_count, datetime, fare_amount = row\n",
    "    \n",
    "    # Validaci√≥n\n",
    "    if (trip_distance is None or trip_distance <= 0 or trip_distance >= 100 or\n",
    "        passenger_count is None or passenger_count <= 0 or passenger_count > 6 or\n",
    "        datetime is None or\n",
    "        fare_amount is None or fare_amount <= 0 or fare_amount >= 200):\n",
    "        return None\n",
    "    \n",
    "    # Extraer features\n",
    "    hour_value = float(datetime.hour)\n",
    "    day_of_week = float(datetime.weekday() + 1)\n",
    "    \n",
    "    # Escalado Z-score\n",
    "    trip_distance_scaled = (trip_distance - 3.0) / 5.0\n",
    "    passenger_count_scaled = (passenger_count - 1.5) / 1.0\n",
    "    hour_scaled = (hour_value - 12.0) / 7.0\n",
    "    day_scaled = (day_of_week - 4.0) / 2.0\n",
    "    \n",
    "    features = [\n",
    "        float(trip_distance_scaled),\n",
    "        float(passenger_count_scaled),\n",
    "        float(hour_scaled),\n",
    "        float(day_scaled)\n",
    "    ]\n",
    "    \n",
    "    return (features, float(fare_amount))\n",
    "\n",
    "print(\"\\nüîÑ Procesamiento distribuido con 8 cores...\")\n",
    "start_processing = time.time()\n",
    "\n",
    "# Crear RDD con selecci√≥n de columnas\n",
    "rdd_features = df.select(\n",
    "    \"trip_distance\",\n",
    "    \"passenger_count\", \n",
    "    \"tpep_pickup_datetime\",\n",
    "    \"fare_amount\"\n",
    ").rdd.map(lambda row: (\n",
    "    row.trip_distance,\n",
    "    row.passenger_count,\n",
    "    row.tpep_pickup_datetime,\n",
    "    row.fare_amount\n",
    "))\n",
    "\n",
    "# Feature engineering distribuido\n",
    "rdd_scaled = rdd_features.map(extract_and_scale_features) \\\n",
    "    .filter(lambda x: x is not None) \\\n",
    "    .repartition(16) \\\n",
    "    .cache()\n",
    "\n",
    "# Forzar evaluaci√≥n\n",
    "total_scaled = rdd_scaled.count()\n",
    "\n",
    "processing_time = time.time() - start_processing\n",
    "\n",
    "print(f\"\\n‚úì Procesamiento completado en {processing_time:.1f}s\")\n",
    "print(f\"  Registros v√°lidos: {total_scaled:,}\")\n",
    "print(f\"  Velocidad: {total_scaled/processing_time:,.0f} registros/segundo\")\n",
    "#---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_test_split",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------Divisi√≥n Train/Test----------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PASO 2: DIVISI√ìN TRAIN/TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Dividir\n",
    "train_rdd, test_rdd = rdd_scaled.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Reparticionar y cachear\n",
    "train_rdd = train_rdd.repartition(16).cache()\n",
    "test_rdd = test_rdd.repartition(8).cache()\n",
    "\n",
    "train_count = train_rdd.count()\n",
    "test_count = test_rdd.count()\n",
    "\n",
    "print(f\"\\n‚úì Divisi√≥n completada\")\n",
    "print(f\"  Train: {train_count:,} registros\")\n",
    "print(f\"  Test:  {test_count:,} registros\")\n",
    "#---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convert_to_numpy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------Conversi√≥n EFICIENTE a NumPy------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PASO 3: CONVERSI√ìN EFICIENTE A NUMPY (Aprovechando 16GB RAM)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüí° Estrategia:\")\n",
    "print(\"   ‚Ä¢ Procesamiento Big Data completado (distribuido en 8 cores)\")\n",
    "print(\"   ‚Ä¢ Conversi√≥n eficiente aprovechando 16GB RAM disponible\")\n",
    "print(\"   ‚Ä¢ Entrenamiento ultra-r√°pido con datos en memoria\")\n",
    "\n",
    "def rdd_to_numpy_parallel(rdd):\n",
    "    \"\"\"\n",
    "    Convierte RDD a numpy usando paralelizaci√≥n m√°xima.\n",
    "    Aprovecha que los datos ya est√°n distribuidos y cacheados.\n",
    "    \"\"\"\n",
    "    # Collect en paralelo (Spark lo hace autom√°ticamente)\n",
    "    data = rdd.collect()\n",
    "    \n",
    "    # Separar features y labels usando list comprehension (r√°pido)\n",
    "    X = np.array([item[0] for item in data], dtype=np.float32)\n",
    "    y = np.array([item[1] for item in data], dtype=np.float32)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "print(\"\\nüì¶ Convirtiendo Train RDD...\")\n",
    "start_train = time.time()\n",
    "X_train, y_train = rdd_to_numpy_parallel(train_rdd)\n",
    "train_time = time.time() - start_train\n",
    "print(f\"   ‚úì Train convertido en {train_time:.1f}s ({train_count/train_time:,.0f} reg/s)\")\n",
    "\n",
    "print(\"\\nüì¶ Convirtiendo Test RDD...\")\n",
    "start_test = time.time()\n",
    "X_test, y_test = rdd_to_numpy_parallel(test_rdd)\n",
    "test_time = time.time() - start_test\n",
    "print(f\"   ‚úì Test convertido en {test_time:.1f}s ({test_count/test_time:,.0f} reg/s)\")\n",
    "\n",
    "print(f\"\\n‚úì Conversi√≥n total: {train_time + test_time:.1f}s\")\n",
    "print(f\"  X_train: {X_train.shape} - {X_train.nbytes / 1024**2:.1f} MB\")\n",
    "print(f\"  X_test:  {X_test.shape} - {X_test.nbytes / 1024**2:.1f} MB\")\n",
    "print(f\"  Total RAM: {(X_train.nbytes + X_test.nbytes + y_train.nbytes + y_test.nbytes) / 1024**2:.1f} MB\")\n",
    "\n",
    "# Liberar RDDs de memoria\n",
    "train_rdd.unpersist()\n",
    "test_rdd.unpersist()\n",
    "rdd_scaled.unpersist()\n",
    "\n",
    "print(\"\\n‚úì RDDs liberados de cache (ya no son necesarios)\")\n",
    "#---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------Modelo Optimizado------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PASO 4: CONSTRUCCI√ìN DEL MODELO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(4,), name='capa_1'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(32, activation='relu', name='capa_2'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(16, activation='relu', name='capa_3'),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Dense(8, activation='relu', name='capa_4'),\n",
    "        \n",
    "        Dense(1, activation='linear', name='salida')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "print(\"\\n‚úì Modelo creado\")\n",
    "model.summary()\n",
    "#---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------Entrenamiento ULTRA-R√ÅPIDO---------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PASO 5: ENTRENAMIENTO OPTIMIZADO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Callbacks para optimizaci√≥n\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Configuraci√≥n optimizada\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 2048  # Batch grande para velocidad (16GB RAM lo permite)\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Configuraci√≥n:\")\n",
    "print(f\"   √âpocas m√°ximas: {EPOCHS}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE} (optimizado para velocidad)\")\n",
    "print(f\"   Validation split: {VALIDATION_SPLIT*100:.0f}%\")\n",
    "print(f\"   Early stopping: S√≠ (patience=3)\")\n",
    "print(f\"   Reduce LR: S√≠ (patience=2)\")\n",
    "\n",
    "print(f\"\\nüí° Ventajas:\")\n",
    "print(f\"   ‚Ä¢ Batch grande = menos iteraciones = m√°s r√°pido\")\n",
    "print(f\"   ‚Ä¢ Datos en RAM = acceso instant√°neo\")\n",
    "print(f\"   ‚Ä¢ Early stopping = detiene si no mejora\")\n",
    "print(f\"   ‚Ä¢ Total batches por √©poca: {train_count // BATCH_SIZE}\")\n",
    "\n",
    "print(\"\\nüéØ Iniciando entrenamiento...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# ENTRENAR (R√ÅPIDO)\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì ENTRENAMIENTO COMPLETADO\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Tiempo total: {training_time/60:.2f} minutos\")\n",
    "print(f\"  Tiempo por √©poca: {training_time/len(history.history['loss']):.1f}s\")\n",
    "print(f\"  √âpocas ejecutadas: {len(history.history['loss'])} de {EPOCHS}\")\n",
    "print(f\"  Mejor val_loss: {min(history.history['val_loss']):.4f}\")\n",
    "\n",
    "print(f\"\\nüöÄ Rendimiento:\")\n",
    "print(f\"   ‚Ä¢ Procesamiento Big Data: ‚úì ({processing_time:.1f}s)\")\n",
    "print(f\"   ‚Ä¢ Conversi√≥n eficiente: ‚úì ({train_time + test_time:.1f}s)\")\n",
    "print(f\"   ‚Ä¢ Entrenamiento r√°pido: ‚úì ({training_time:.1f}s)\")\n",
    "print(f\"   ‚Ä¢ Total: {processing_time + train_time + test_time + training_time:.1f}s\")\n",
    "#---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------Evaluaci√≥n-------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PASO 6: EVALUACI√ìN COMPLETA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä Evaluando modelo...\")\n",
    "\n",
    "# Evaluaci√≥n\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Predicciones\n",
    "print(\"üîÆ Generando predicciones...\")\n",
    "y_pred = model.predict(X_test, batch_size=4096, verbose=0)\n",
    "\n",
    "# M√©tricas\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred.flatten()) / y_test)) * 100\n",
    "percent_errors = np.abs((y_test - y_pred.flatten()) / y_test) * 100\n",
    "accuracy_10pct = np.mean(percent_errors <= 10) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTADOS FINALES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìà M√©tricas de Regresi√≥n:\")\n",
    "print(f\"   MSE:  {mse:.4f}\")\n",
    "print(f\"   RMSE: ${rmse:.4f}\")\n",
    "print(f\"   MAE:  ${mae:.4f}\")\n",
    "print(f\"   R¬≤:   {r2:.4f} ({r2*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nüìä M√©tricas de Negocio:\")\n",
    "print(f\"   MAPE:         {mape:.2f}%\")\n",
    "print(f\"   Accuracy@10%: {accuracy_10pct:.2f}%\")\n",
    "\n",
    "print(\"\\nüí° Interpretaci√≥n:\")\n",
    "if r2 > 0.80:\n",
    "    print(f\"   ‚úì EXCELENTE - Modelo de muy alta calidad\")\n",
    "elif r2 > 0.70:\n",
    "    print(f\"   ‚úì MUY BUENO - Modelo s√≥lido\")\n",
    "elif r2 > 0.60:\n",
    "    print(f\"   ‚úì BUENO - Modelo aceptable\")\n",
    "else:\n",
    "    print(f\"   ‚ö† MODERADO - Considerar mejoras\")\n",
    "\n",
    "print(f\"\\n   Error promedio: ¬±${mae:.2f} (¬±{mape:.1f}%)\")\n",
    "print(f\"   {accuracy_10pct:.0f}% predicciones dentro de ¬±10%\")\n",
    "#---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------Ejemplos---------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EJEMPLOS DE PREDICCIONES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sample_indices = np.random.choice(len(y_test), size=20, replace=False)\n",
    "sample_real = y_test[sample_indices]\n",
    "sample_pred = y_pred[sample_indices].flatten()\n",
    "\n",
    "print(\"\\nüîç 20 ejemplos:\\n\")\n",
    "print(f\"{'Predicci√≥n':<15} {'Real':<15} {'Error':<15} {'Error %':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for pred, real in zip(sample_pred, sample_real):\n",
    "    error = pred - real\n",
    "    error_pct = (error / real) * 100 if real != 0 else 0\n",
    "    print(f\"${pred:<14.2f} ${real:<14.2f} ${error:<14.2f} {error_pct:<14.1f}%\")\n",
    "\n",
    "all_errors = y_pred.flatten() - y_test\n",
    "print(f\"\\nüìä Estad√≠sticas:\")\n",
    "print(f\"   Error min:  ${np.min(all_errors):.2f}\")\n",
    "print(f\"   Error max:  ${np.max(all_errors):.2f}\")\n",
    "print(f\"   Error mean: ${np.mean(all_errors):.2f}\")\n",
    "print(f\"   Error std:  ${np.std(all_errors):.2f}\")\n",
    "#---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------Guardar Modelo---------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GUARDAR MODELO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "os.makedirs(\"modelos\", exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_path = f\"modelos/taxi_fare_OPTIMIZADO_{timestamp}.h5\"\n",
    "model.save(model_path)\n",
    "\n",
    "print(f\"\\n‚úì Modelo guardado: {model_path}\")\n",
    "print(f\"  Tama√±o: {os.path.getsize(model_path) / 1024:.2f} KB\")\n",
    "print(f\"  R¬≤ Score: {r2:.4f}\")\n",
    "#---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------Resumen Final----------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN FINAL - VERSI√ìN OPTIMIZADA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_time = processing_time + train_time + test_time + training_time\n",
    "\n",
    "summary = f\"\"\"\n",
    "üöÄ OPTIMIZACI√ìN PARA 16GB RAM + 8 CORES:\n",
    "   ‚úì Procesamiento Big Data distribuido (8 cores)\n",
    "   ‚úì Conversi√≥n eficiente aprovechando RAM\n",
    "   ‚úì Entrenamiento ultra-r√°pido (batch_size=2048)\n",
    "   ‚úì Toda la base de datos procesada: {total_scaled:,} registros\n",
    "\n",
    "‚è±Ô∏è  TIEMPOS DE EJECUCI√ìN:\n",
    "   ‚Ä¢ Procesamiento Spark:  {processing_time:.1f}s\n",
    "   ‚Ä¢ Conversi√≥n a numpy:   {train_time + test_time:.1f}s\n",
    "   ‚Ä¢ Entrenamiento:        {training_time:.1f}s ({training_time/60:.1f} min)\n",
    "   ‚Ä¢ TOTAL:                {total_time:.1f}s ({total_time/60:.1f} min)\n",
    "\n",
    "üìä DATOS:\n",
    "   ‚Ä¢ Dataset completo: {total_scaled:,} registros\n",
    "   ‚Ä¢ Train: {train_count:,}\n",
    "   ‚Ä¢ Test: {test_count:,}\n",
    "   ‚Ä¢ Sin muestreo ni subconjuntos\n",
    "\n",
    "üèóÔ∏è  MODELO:\n",
    "   ‚Ä¢ Arquitectura: 64-32-16-8-1\n",
    "   ‚Ä¢ Par√°metros: {model.count_params():,}\n",
    "   ‚Ä¢ √âpocas: {len(history.history['loss'])}\n",
    "   ‚Ä¢ Batch size: {BATCH_SIZE}\n",
    "\n",
    "üìà RESULTADOS:\n",
    "   ‚Ä¢ R¬≤:   {r2:.4f} ({r2*100:.1f}%)\n",
    "   ‚Ä¢ RMSE: ${rmse:.4f}\n",
    "   ‚Ä¢ MAE:  ${mae:.4f}\n",
    "   ‚Ä¢ MAPE: {mape:.2f}%\n",
    "   ‚Ä¢ Accuracy@10%: {accuracy_10pct:.2f}%\n",
    "\n",
    "üíæ GUARDADO:\n",
    "   ‚Ä¢ {model_path}\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ TAREA COMPLETADA - VERSI√ìN OPTIMIZADA PARA VELOCIDAD\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüéØ Tiempo total: {total_time/60:.1f} minutos (vs >10 min anterior)\")\n",
    "print(f\"   Mejora: ~{(600/total_time):.1f}x m√°s r√°pido\")\n",
    "#---------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
