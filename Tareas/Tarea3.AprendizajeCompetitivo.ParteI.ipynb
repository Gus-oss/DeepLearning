{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b010671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------- Librerias ----------------------------------------------\n",
    "#Librerias tipicas para el análisis de datos\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Librerias para la implementación de pyspark\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\hadoop\"\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Libreraias para la implementación de keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Layer, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acfdf3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sessión de Spark creada\n",
      "  Network timeout: 800s\n",
      "  Worker timeout: 600s\n"
     ]
    }
   ],
   "source": [
    "#-----------------------SparkSession-----------------------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC_Taxi_Spark_DeepLearning\") \\\n",
    "    .master(\"local[8]\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .config(\"spark.python.worker.timeout\", \"600s\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"16\") \\\n",
    "    .config(\"spark.default.parallelism\", \"16\") \\\n",
    "    .config(\"spark.rdd.compress\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\" Sessión de Spark creada\")\n",
    "print(\"  Network timeout: 800s\")\n",
    "print(\"  Worker timeout: 600s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c417af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Cargando dataset\n",
      " Numero de registros: 2,964,624\n",
      " Columnas: 19\n"
     ]
    }
   ],
   "source": [
    "#----------------------Cargar datos-----------------------------------------------\n",
    "DATA_PATH = \"C:/Users/PC/Documents/DocumentosGustavo/Github/Maestria/BigData/nyc-taxi-spark/data/yellow/2024/yellow_tripdata_2024-01.parquet\"\n",
    "\n",
    "print(\"\\n Cargando dataset\")\n",
    "df = spark.read.parquet(DATA_PATH)\n",
    "print(f\" Numero de registros: {df.count():,}\")\n",
    "print(f\" Columnas: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f232ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Esquema del dataset:\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEsquema del dataset:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06500a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeros 5 registros\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2024-01-01 00:57:55|  2024-01-01 01:17:43|              1|         1.72|         1|                 N|         186|          79|           2|       17.7|  1.0|    0.5|       0.0|         0.0|                  1.0|        22.7|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:03:00|  2024-01-01 00:09:36|              1|          1.8|         1|                 N|         140|         236|           1|       10.0|  3.5|    0.5|      3.75|         0.0|                  1.0|       18.75|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:17:06|  2024-01-01 00:35:01|              1|          4.7|         1|                 N|         236|          79|           1|       23.3|  3.5|    0.5|       3.0|         0.0|                  1.0|        31.3|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:36:38|  2024-01-01 00:44:56|              1|          1.4|         1|                 N|          79|         211|           1|       10.0|  3.5|    0.5|       2.0|         0.0|                  1.0|        17.0|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:46:51|  2024-01-01 00:52:57|              1|          0.8|         1|                 N|         211|         148|           1|        7.9|  3.5|    0.5|       3.2|         0.0|                  1.0|        16.1|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"Primeros 5 registros\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b914f4",
   "metadata": {},
   "source": [
    "Para la implementación de esta tarea se elegiran las variables: \n",
    "- trip_distance\n",
    "- passenger_count\n",
    "- tpep_pickup_datetime\n",
    "- fare_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f27e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando features\n",
      "Registros despues de la limpieza: 2,722,784 registros. \n",
      " Completado en 58.7s.\n"
     ]
    }
   ],
   "source": [
    "#----------------------Feature Engineering----------------------------------------\n",
    "def extract_and_scale_features(row):\n",
    "    trip_distance, passenger_count, datetime, fare_amount = row\n",
    "    \n",
    "    if (trip_distance is None or trip_distance <= 0 or trip_distance >= 100 or\n",
    "        passenger_count is None or passenger_count <= 0 or passenger_count > 6 or\n",
    "        datetime is None or\n",
    "        fare_amount is None or fare_amount <= 0 or fare_amount >= 200):\n",
    "        return None\n",
    "    \"\"\"\n",
    "    trip_distance <= 0: Viajes inválidos (errores de sensor).\n",
    "    trip_distance >= 100: Outliers extremos (probablemente errores).\n",
    "    passenger_count <= 0 or > 6: NYC taxis tienen máximo 5 pasajeros + 1 niño.\n",
    "    fare_amount <= 0 or >= 200: Errores de medición o fraudes.\n",
    "\n",
    "    \"\"\"\n",
    "    hour_value = float(datetime.hour) #hora del día: 0-23\n",
    "    day_of_week = float(datetime.weekday() + 1) #dia de la semana: L-D\n",
    "    \n",
    "    features = [\n",
    "        float((trip_distance - 3.0) / 5.0),\n",
    "        float((passenger_count - 1.5) / 1.0),\n",
    "        float((hour_value - 12.0) / 7.0),\n",
    "        float((day_of_week - 4.0) / 2.0)\n",
    "    ]\n",
    "    \"\"\"\n",
    "    Normalización:  z = (x - μ) / σ, esto por que \n",
    "    Convergencia: Gradientes similares en todas las features.\n",
    "    Velocidad: Adam converge más rápido con datos escalados.\n",
    "    Estabilidad numérica: Evita overflow/underflow.\n",
    "    ReLU: Funciona mejor con datos centrados en 0.\n",
    "\n",
    "    \"\"\"\n",
    "    return (features, float(fare_amount)) #Estructura: Tupla (lista_features, label). Compatible con RDD map-reduce y Keras.\n",
    "\n",
    "print(\"\\nProcesando features\")\n",
    "start = time.time()\n",
    "\n",
    "rdd_features = df.select( #convierte a rdd\n",
    "    \"trip_distance\", \"passenger_count\", \"tpep_pickup_datetime\", \"fare_amount\"\n",
    ").rdd.map(lambda row: (\n",
    "    row.trip_distance, row.passenger_count, row.tpep_pickup_datetime, row.fare_amount # Transformación 1-a-1. Ejecución: Lazy (no se ejecuta hasta una acción).\n",
    ")) #Variables que se eligieron\n",
    "\n",
    "rdd_scaled = rdd_features \\\n",
    "    .map(extract_and_scale_features) \\\n",
    "    .filter(lambda x: x is not None) \\\n",
    "    .repartition(16) \\\n",
    "    .cache()\n",
    "# lambda x:x is not None:  Elimina registros None (inválidos)\n",
    "# .repartition(16): Redistribuir datos en 16 particiones balanceadas\n",
    "total_scaled = rdd_scaled.count()\n",
    "print(f\"Registros despues de la limpieza: {total_scaled:,} registros. \\n Completado en {time.time()-start:.1f}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9e3cc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train: 2,178,814 registros\n",
      " Test: 543,970 registros\n"
     ]
    }
   ],
   "source": [
    "#----------------------División Train/Test----------------------------------------\n",
    "train_rdd, test_rdd = rdd_scaled.randomSplit([0.8, 0.2]) #80% entrenamiento y 20% prueba aleatorio \n",
    "\n",
    "from pyspark import StorageLevel\n",
    "train_rdd = train_rdd.repartition(16).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "test_rdd = test_rdd.repartition(8).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "#StorageLevel.MEMORY_AND_DISK: primer intenta usar memoria, sino cabe utiliza disco\n",
    "\n",
    "train_count = train_rdd.count()\n",
    "test_count = test_rdd.count()\n",
    "\n",
    "print(f\"\\n Train: {train_count:,} registros\")\n",
    "print(f\" Test: {test_count:,} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b5d014",
   "metadata": {},
   "source": [
    "Para este trabajo, utilizaremos \n",
    "$$ E_{ck} = \\frac{1}{N}\\sum_{k=1}^{K} \\sum_{x_{i} \\in C_{k}} \\mu_{kp} \\lvert x_{n} - c_{k} \\rvert^{2}$$\n",
    "La arquitectura a trabajar sera la siguiente: \n",
    "- Input (batch, 4 variables)\n",
    "- CompetitiveLearningLayer: $\\lvert x_{n} - c_{k} \\rvert^{2}$ que son las distancias. Despues $\\mu_{kp} = softmax(-distancia)$ la cual suavisa la competición (batch, 64 neuronas)\n",
    "- Capa oculta 1: 32 neuronas con activacion relu\n",
    "- Capa oculta 2: 16 neuronas con activacion relu\n",
    "- Capa oculta 3:  8 neuronas con activacion relu\n",
    "- Capa salida  :  1 neurona con activacion lineal.\n",
    "- Optimizador: Adam\n",
    "- Función de costo: MSE\n",
    "- Metrica adicional: MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9627ae78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Configuración de Learning Rate:\n",
      "   Fórmula: η(t) = η₀(1 - t/T)\n",
      "   η₀ (inicial): 0.001\n",
      "   T (total steps): 6000 = 15 épocas × 400 batches\n",
      "   Decaimiento: lineal de 0.001 → ~0\n",
      "\n",
      " Modelo con Aprendizaje Competitivo y LR Variable creado\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ competitive_learning_layer_1    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CompetitiveLearningLayer</span>)      │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ competitive_learning_layer_1    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mCompetitiveLearningLayer\u001b[0m)      │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m9\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,393</span> (13.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,393\u001b[0m (13.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,201</span> (12.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,201\u001b[0m (12.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#----------------------Capa de Aprendizaje Competitivo--------------------\n",
    "class CompetitiveLearningLayer(Layer):\n",
    "    \"\"\"\n",
    "    Implementa la función de costo competitivo:\n",
    "        E_ck = (1/N) * sum_k sum_{x_i in C_k} mu_kp * ||x_n - c_k||^2\n",
    "\n",
    "    donde:\n",
    "      - c_k  : centros de cluster \n",
    "      - mu_kp: se modela mediante soft-competition \n",
    "               mu_kp = softmax(-||x_n - c_k||^2)\n",
    "               Neurona ganadora -> cercana a 1\n",
    "               Neuronas perdedoras ->  cercana a 0\n",
    "    Competicion relajada\n",
    "    Flujo de información:\n",
    "      input (batch, 4) --> distancias (batch, K) --> membresías (batch, K)\n",
    "\n",
    "    Los gradientes del MSE final fluyen hacia atrás actualizando tanto\n",
    "    los centros c_k como los pesos de las capas Dense posteriores.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters, **kwargs):\n",
    "        super(CompetitiveLearningLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n_features = int(input_shape[-1])\n",
    "        # Centros de cluster c_k: forma (K, n_features)\n",
    "        # Inicialización glorot para estabilidad del gradiente\n",
    "        self.cluster_centers = self.add_weight(\n",
    "            name='cluster_centers',\n",
    "            shape=(self.n_clusters, n_features),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        super(CompetitiveLearningLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs: (batch, n_features)\n",
    "\n",
    "        # Expandir dimensiones para broadcast\n",
    "        # x_expanded: (batch, 1, n_features)\n",
    "        # c_expanded: (1, K, n_features)\n",
    "        x_expanded = tf.expand_dims(inputs, axis=1)\n",
    "        c_expanded = tf.expand_dims(self.cluster_centers, axis=0)\n",
    "\n",
    "        # Distancias cuadradas ||x_n - c_k||^2: (batch, K)\n",
    "        distances_sq = tf.reduce_sum(tf.square(x_expanded - c_expanded), axis=-1)\n",
    "\n",
    "        # Membresías mu_kp = softmax(-distancias)\n",
    "        # La neurona mas cercana obtiene la mayor membresía -> competición suave\n",
    "        # El signo negativo invierte: menor distancia = mayor activación\n",
    "        memberships = tf.nn.softmax(-distances_sq, axis=-1)  # (batch, K)\n",
    "\n",
    "        return memberships\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'n_clusters': self.n_clusters})\n",
    "        return config\n",
    "\n",
    "\n",
    "#----------------------Tasa de Aprendizaje Variable-----------------------\n",
    "class LinearDecayLR(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    Implementa: η(t) = η₀(1 - t/T)\n",
    "    \n",
    "    donde:\n",
    "        η₀: tasa de aprendizaje inicial\n",
    "        t:  paso/batch actual (0, 1, 2, ..., T-1)\n",
    "        T:  número total de pasos (epochs × batches_per_epoch)\n",
    "    \n",
    "    La tasa decrece linealmente desde η₀ hasta ~0 a medida que\n",
    "    el entrenamiento avanza de t=0 a t=T.\n",
    "    \"\"\"\n",
    "    def __init__(self, initial_learning_rate, total_steps):\n",
    "        super(LinearDecayLR, self).__init__()\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.total_steps = total_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        # η(t) = η₀(1 - t/T)\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        total_steps = tf.cast(self.total_steps, tf.float32)\n",
    "        decay_factor = 1.0 - (step / total_steps)\n",
    "        # Evitar que la tasa llegue exactamente a 0 (min = 1e-7)\n",
    "        decay_factor = tf.maximum(decay_factor, 1e-7 / self.initial_learning_rate)\n",
    "        return self.initial_learning_rate * decay_factor\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'initial_learning_rate': self.initial_learning_rate,\n",
    "            'total_steps': self.total_steps\n",
    "        }\n",
    "\n",
    "\n",
    "#----------------------Modelo con Aprendizaje Competitivo-----------------\n",
    "def create_model(epochs=15, batches_per_epoch=400):\n",
    "    \"\"\"\n",
    "\n",
    "    La primera capa Dense(64, relu) es reemplazada por CompetitiveLearningLayer(64)\n",
    "    que implementa competición suave entre K=64 centros de cluster.\n",
    "    El resto de la arquitectura y funciones de costo se conservan\n",
    "    de la tarea de big data y deep learning.\n",
    "    \n",
    "    Tasa de aprendizaje variable con decaimiento lineal\n",
    "        η(t) = η₀(1 - t/T)\n",
    "        donde η₀=0.001, T=epochs×batches_per_epoch\n",
    "    \"\"\"\n",
    "    # Configuración del learning rate con decaimiento lineal\n",
    "    initial_lr = 0.001  # η₀\n",
    "    total_steps = epochs * batches_per_epoch  # T\n",
    "    \n",
    "    lr_schedule = LinearDecayLR(\n",
    "        initial_learning_rate=initial_lr,\n",
    "        total_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    model = Sequential([\n",
    "        # --- Capa 1: Aprendizaje Competitivo con Promedio (K=64 clusters) ---\n",
    "        # Parámetros entrenables: 64 * 4 = 256 (centros c_k)\n",
    "        # Salida: vector de 64 membresías mu_kp en [0, 1] con suma = 1\n",
    "        CompetitiveLearningLayer(64, input_shape=(4,)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        # --- Capas de regresión (conservadas del modelo original) ---\n",
    "        Dense(32, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "    # Funciones de costo conservadas: MSE + MAE\n",
    "    # Optimizador Adam CON tasa de aprendizaje variable\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=lr_schedule),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n Configuración de Learning Rate:\")\n",
    "    print(f\"   Fórmula: η(t) = η₀(1 - t/T)\")\n",
    "    print(f\"   η₀ (inicial): {initial_lr}\")\n",
    "    print(f\"   T (total steps): {total_steps} = {epochs} épocas × {batches_per_epoch} batches\")\n",
    "    print(f\"   Decaimiento: lineal de {initial_lr} → ~0\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear modelo con parámetros de entrenamiento\n",
    "EPOCHS = 15\n",
    "BATCHES_PER_EPOCH = 400\n",
    "model = create_model(epochs=EPOCHS, batches_per_epoch=BATCHES_PER_EPOCH)\n",
    "\n",
    "print(\"\\n Modelo con Aprendizaje Competitivo y LR Variable creado\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdbd09d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch size: 4096\n",
      "  Batches/época train: 400\n",
      "  Samples/época: 1,638,400\n"
     ]
    }
   ],
   "source": [
    "#----------------------Generador de batches--------------------------------\n",
    "class RobustRDDBatchGenerator: #Generador que usa toLocalIterator().\n",
    "    \n",
    "    def __init__(self, rdd, batch_size=4096, num_batches_per_epoch=None):\n",
    "        self.rdd = rdd\n",
    "        self.batch_size = batch_size\n",
    "        self.total_samples = rdd.count()\n",
    "        \n",
    "        if num_batches_per_epoch:\n",
    "            self.num_batches = num_batches_per_epoch\n",
    "        else:\n",
    "            self.num_batches = max(1, self.total_samples // batch_size)\n",
    "    \n",
    "    def generate_batches(self, seed=42):\n",
    "        \"\"\"\n",
    "        Genera batches usando toLocalIterator.\n",
    "        \n",
    "        toLocalIterator:\n",
    "        - Itera sobre RDD SIN collect masivo\n",
    "        - No causa timeout\n",
    "        - Procesa partición por partición\n",
    "        - 100% RDD distribuido\n",
    "        \"\"\"\n",
    "        # Sample del RDD\n",
    "        fraction = min(1.0, (self.batch_size * self.num_batches) / self.total_samples)\n",
    "        sampled_rdd = self.rdd.sample(False, fraction, seed=seed)\n",
    "        \n",
    "        # Usar toLocalIterator\n",
    "        batch_data = []\n",
    "        batch_count = 0\n",
    "        \n",
    "        \n",
    "        for item in sampled_rdd.toLocalIterator():\n",
    "            batch_data.append(item)\n",
    "            \n",
    "            # Cuando el batch está lleno, yield\n",
    "            if len(batch_data) >= self.batch_size:\n",
    "                X_batch = np.array([x[0] for x in batch_data], dtype=np.float32)\n",
    "                y_batch = np.array([x[1] for x in batch_data], dtype=np.float32)\n",
    "                \n",
    "                yield X_batch, y_batch\n",
    "                \n",
    "                batch_data = []\n",
    "                batch_count += 1\n",
    "                \n",
    "                # Limitar número de batches\n",
    "                if batch_count >= self.num_batches:\n",
    "                    break\n",
    "        \n",
    "        # Último batch parcial\n",
    "        if batch_data and batch_count < self.num_batches:\n",
    "            X_batch = np.array([x[0] for x in batch_data], dtype=np.float32)\n",
    "            y_batch = np.array([x[1] for x in batch_data], dtype=np.float32)\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "# Configuración\n",
    "BATCH_SIZE = 4096  \n",
    "BATCHES_PER_EPOCH_TRAIN = 400\n",
    "BATCHES_PER_EPOCH_VAL = 20\n",
    "\n",
    "train_generator = RobustRDDBatchGenerator(\n",
    "    train_rdd, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_batches_per_epoch=BATCHES_PER_EPOCH_TRAIN\n",
    ")\n",
    "\n",
    "test_generator = RobustRDDBatchGenerator(\n",
    "    test_rdd,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_batches_per_epoch=BATCHES_PER_EPOCH_VAL\n",
    ")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Batches/época train: {BATCHES_PER_EPOCH_TRAIN}\")\n",
    "print(f\"  Samples/época: {BATCH_SIZE * BATCHES_PER_EPOCH_TRAIN:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92f77e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento.\n",
      "\n",
      "   Configuración:\n",
      "   Épocas: 15\n",
      "   Batches/época: 400\n",
      "   Batch size: 4096\n",
      "\n",
      "  Iniciando el entrenamiento\n",
      "\n",
      "\n",
      "Época 1/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 354.8284\n",
      "  Batch 200/400 - loss: 200.0264\n",
      "  Batch 300/400 - loss: 145.2214\n",
      "  Batch 400/400 - loss: 117.4939\n",
      "\n",
      "    Época 1:\n",
      "     loss: 252.5377 - mae: 10.2105\n",
      "     val_loss: 116.9337 - val_mae: 6.3361\n",
      "     Tiempo: 49.6s\n",
      "\n",
      "Época 2/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 103.5262\n",
      "  Batch 200/400 - loss: 92.3970\n",
      "  Batch 300/400 - loss: 84.1779\n",
      "  Batch 400/400 - loss: 77.8937\n",
      "\n",
      "    Época 2:\n",
      "     loss: 93.4163 - mae: 5.4792\n",
      "     val_loss: 76.7775 - val_mae: 4.8446\n",
      "     Tiempo: 47.3s\n",
      "\n",
      "Época 3/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 72.0168\n",
      "  Batch 200/400 - loss: 68.2198\n",
      "  Batch 300/400 - loss: 65.0250\n",
      "  Batch 400/400 - loss: 62.2472\n",
      "\n",
      "    Época 3:\n",
      "     loss: 68.2709 - mae: 4.5063\n",
      "     val_loss: 61.7021 - val_mae: 4.2369\n",
      "     Tiempo: 47.7s\n",
      "\n",
      "Época 4/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 59.3989\n",
      "  Batch 200/400 - loss: 57.4770\n",
      "  Batch 300/400 - loss: 55.7665\n",
      "  Batch 400/400 - loss: 54.2040\n",
      "\n",
      "    Época 4:\n",
      "     loss: 57.4387 - mae: 4.0554\n",
      "     val_loss: 53.8826 - val_mae: 3.9021\n",
      "     Tiempo: 47.5s\n",
      "\n",
      "Época 5/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 52.5143\n",
      "  Batch 200/400 - loss: 51.3586\n",
      "  Batch 300/400 - loss: 50.2944\n",
      "  Batch 400/400 - loss: 49.2809\n",
      "\n",
      "    Época 5:\n",
      "     loss: 51.3082 - mae: 3.7873\n",
      "     val_loss: 49.0734 - val_mae: 3.6866\n",
      "     Tiempo: 51.0s\n",
      "\n",
      "Época 6/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 48.1327\n",
      "  Batch 200/400 - loss: 47.3337\n",
      "  Batch 300/400 - loss: 46.5848\n",
      "  Batch 400/400 - loss: 45.8685\n",
      "\n",
      "    Época 6:\n",
      "     loss: 47.2957 - mae: 3.6053\n",
      "     val_loss: 45.7276 - val_mae: 3.5324\n",
      "     Tiempo: 58.9s\n",
      "\n",
      "Época 7/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 45.0484\n",
      "  Batch 200/400 - loss: 44.4604\n",
      "  Batch 300/400 - loss: 43.8923\n",
      "  Batch 400/400 - loss: 43.3464\n",
      "\n",
      "    Época 7:\n",
      "     loss: 44.4189 - mae: 3.4709\n",
      "     val_loss: 43.2353 - val_mae: 3.4146\n",
      "     Tiempo: 59.7s\n",
      "\n",
      "Época 8/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 42.7234\n",
      "  Batch 200/400 - loss: 42.2757\n",
      "  Batch 300/400 - loss: 41.8362\n",
      "  Batch 400/400 - loss: 41.3916\n",
      "\n",
      "    Época 8:\n",
      "     loss: 42.2381 - mae: 3.3659\n",
      "     val_loss: 41.3047 - val_mae: 3.3203\n",
      "     Tiempo: 68.2s\n",
      "\n",
      "Época 9/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 40.8921\n",
      "  Batch 200/400 - loss: 40.5183\n",
      "  Batch 300/400 - loss: 40.1618\n",
      "  Batch 400/400 - loss: 39.8079\n",
      "\n",
      "    Época 9:\n",
      "     loss: 40.4933 - mae: 3.2802\n",
      "     val_loss: 39.7416 - val_mae: 3.2427\n",
      "     Tiempo: 64.8s\n",
      "\n",
      "Época 10/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 39.4137\n",
      "  Batch 200/400 - loss: 39.1143\n",
      "  Batch 300/400 - loss: 38.8309\n",
      "  Batch 400/400 - loss: 38.5316\n",
      "\n",
      "    Época 10:\n",
      "     loss: 39.0934 - mae: 3.2097\n",
      "     val_loss: 38.4801 - val_mae: 3.1784\n",
      "     Tiempo: 55.6s\n",
      "\n",
      "Época 11/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 38.2080\n",
      "  Batch 200/400 - loss: 37.9560\n",
      "  Batch 300/400 - loss: 37.7055\n",
      "  Batch 400/400 - loss: 37.4646\n",
      "\n",
      "    Época 11:\n",
      "     loss: 37.9347 - mae: 3.1506\n",
      "     val_loss: 37.4241 - val_mae: 3.1242\n",
      "     Tiempo: 60.5s\n",
      "\n",
      "Época 12/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 37.1925\n",
      "  Batch 200/400 - loss: 36.9797\n",
      "  Batch 300/400 - loss: 36.7730\n",
      "  Batch 400/400 - loss: 36.5571\n",
      "\n",
      "    Época 12:\n",
      "     loss: 36.9618 - mae: 3.1004\n",
      "     val_loss: 36.5213 - val_mae: 3.0776\n",
      "     Tiempo: 63.5s\n",
      "\n",
      "Época 13/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 36.3248\n",
      "  Batch 200/400 - loss: 36.1437\n",
      "  Batch 300/400 - loss: 35.9658\n",
      "  Batch 400/400 - loss: 35.7814\n",
      "\n",
      "    Época 13:\n",
      "     loss: 36.1264 - mae: 3.0572\n",
      "     val_loss: 35.7490 - val_mae: 3.0377\n",
      "     Tiempo: 56.0s\n",
      "\n",
      "Época 14/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 35.5813\n",
      "  Batch 200/400 - loss: 35.4257\n",
      "  Batch 300/400 - loss: 35.2715\n",
      "  Batch 400/400 - loss: 35.1110\n",
      "\n",
      "    Época 14:\n",
      "     loss: 35.4107 - mae: 3.0201\n",
      "     val_loss: 35.0820 - val_mae: 3.0032\n",
      "     Tiempo: 59.4s\n",
      "\n",
      "Época 15/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 34.9351\n",
      "  Batch 200/400 - loss: 34.7981\n",
      "  Batch 300/400 - loss: 34.6628\n",
      "  Batch 400/400 - loss: 34.5209\n",
      "\n",
      "    Época 15:\n",
      "     loss: 34.7847 - mae: 2.9878\n",
      "     val_loss: 34.4976 - val_mae: 2.9730\n",
      "     Tiempo: 56.5s\n",
      "\n",
      "================================================================================\n",
      "✓ Entrenamiento completo\n",
      "================================================================================\n",
      "  Tiempo: 14.10 minutos\n",
      "  Épocas: 15\n",
      "  Mejor val_loss: 34.4976\n"
     ]
    }
   ],
   "source": [
    "#----------------------Entrenamiento----------------------------------------------\n",
    "print(\"Entrenamiento.\")\n",
    "\n",
    "EPOCHS = 15\n",
    "\n",
    "print(f\"\\n   Configuración:\")\n",
    "print(f\"   Épocas: {EPOCHS}\")\n",
    "print(f\"   Batches/época: {BATCHES_PER_EPOCH_TRAIN}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "history = {'loss': [], 'mae': [], 'val_loss': [], 'val_mae': []}\n",
    "\n",
    "print(\"\\n  Iniciando el entrenamiento\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    print(f\"\\nÉpoca {epoch+1}/{EPOCHS}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    epoch_losses = []\n",
    "    epoch_maes = []\n",
    "    \n",
    "    # Entrenar\n",
    "    batch_count = 0\n",
    "    try:\n",
    "        for X_batch, y_batch in train_generator.generate_batches(seed=epoch):\n",
    "            metrics = model.train_on_batch(X_batch, y_batch, return_dict=True)\n",
    "            epoch_losses.append(metrics['loss'])\n",
    "            epoch_maes.append(metrics['mae'])\n",
    "            batch_count += 1\n",
    "            \n",
    "            if batch_count % 100 == 0:\n",
    "                print(f\"  Batch {batch_count}/{BATCHES_PER_EPOCH_TRAIN} - \"\n",
    "                      f\"loss: {np.mean(epoch_losses[-20:]):.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error en batch {batch_count}: {e}\")\n",
    "        print(f\"  Siguiente epoca\")\n",
    "        continue\n",
    "    \n",
    "    if not epoch_losses:\n",
    "        print(\"  No se completaron batches, saltando época\")\n",
    "        continue\n",
    "    \n",
    "    train_loss = np.mean(epoch_losses)\n",
    "    train_mae = np.mean(epoch_maes)\n",
    "    \n",
    "    # Validación\n",
    "    val_losses = []\n",
    "    val_maes = []\n",
    "    for X_val, y_val in test_generator.generate_batches(seed=epoch):\n",
    "        val_metrics = model.test_on_batch(X_val, y_val, return_dict=True)\n",
    "        val_losses.append(val_metrics['loss'])\n",
    "        val_maes.append(val_metrics['mae'])\n",
    "    \n",
    "    val_loss = np.mean(val_losses) if val_losses else train_loss\n",
    "    val_mae = np.mean(val_maes) if val_maes else train_mae\n",
    "    \n",
    "    history['loss'].append(train_loss)\n",
    "    history['mae'].append(train_mae)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_mae'].append(val_mae)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"\\n    Época {epoch+1}:\")\n",
    "    print(f\"     loss: {train_loss:.4f} - mae: {train_mae:.4f}\")\n",
    "    print(f\"     val_loss: {val_loss:.4f} - val_mae: {val_mae:.4f}\")\n",
    "    print(f\"     Tiempo: {epoch_time:.1f}s\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if epoch > 3 and val_loss > history['val_loss'][-2]:\n",
    "        patience = getattr(model, 'patience', 0) + 1\n",
    "        model.patience = patience\n",
    "        if patience >= 3:\n",
    "            print(f\"\\n   Early stopping\")\n",
    "            break\n",
    "    else:\n",
    "        model.patience = 0\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Entrenamiento completo\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Tiempo: {training_time/60:.2f} minutos\")\n",
    "print(f\"  Épocas: {len(history['loss'])}\")\n",
    "print(f\"  Mejor val_loss: {min(history['val_loss']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51e68950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluando modelo.\n",
      "Resultados\n",
      "\n",
      "  R²:   0.9003 (90.0%)\n",
      "  RMSE: $5.2321\n",
      "  MAE:  $2.4113\n",
      "  MAPE: 16.28%\n",
      "\n",
      "  Evaluado en 409,193 predicciones\n"
     ]
    }
   ],
   "source": [
    "#----------------------Evaluación-------------------------------------------------\n",
    "print(\"\\nEvaluando modelo.\")\n",
    "\n",
    "eval_generator = RobustRDDBatchGenerator(\n",
    "    test_rdd,\n",
    "    batch_size=4096,\n",
    "    num_batches_per_epoch=100\n",
    ")\n",
    "\n",
    "all_predictions = []\n",
    "all_actuals = []\n",
    "\n",
    "for X_test, y_test_batch in eval_generator.generate_batches(seed=99):\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    all_predictions.extend(y_pred.flatten().tolist())\n",
    "    all_actuals.extend(y_test_batch.tolist())\n",
    "\n",
    "y_test_eval = np.array(all_actuals)\n",
    "y_pred_eval = np.array(all_predictions)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "mse = mean_squared_error(y_test_eval, y_pred_eval)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_eval, y_pred_eval)\n",
    "r2 = r2_score(y_test_eval, y_pred_eval)\n",
    "mape = np.mean(np.abs((y_test_eval - y_pred_eval) / y_test_eval)) * 100\n",
    "\n",
    "print(\"Resultados\")\n",
    "print(f\"\\n  R²:   {r2:.4f} ({r2*100:.1f}%)\")\n",
    "print(f\"  RMSE: ${rmse:.4f}\")\n",
    "print(f\"  MAE:  ${mae:.4f}\")\n",
    "print(f\"  MAPE: {mape:.2f}%\")\n",
    "print(f\"\\n  Evaluado en {len(y_test_eval):,} predicciones\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
