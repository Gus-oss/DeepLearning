{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be3e353c",
   "metadata": {},
   "source": [
    "# Tarea 2 - Red Neuronal con Elephas (100% Big Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "library_section",
   "metadata": {},
   "source": [
    "## 1. Importar Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================= LIBRERÍAS =============================\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración de Spark/Hadoop\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\hadoop\"\n",
    "\n",
    "# PySpark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import col, rand\n",
    "from pyspark.sql.types import DoubleType, StructType, StructField\n",
    "\n",
    "# Keras/TensorFlow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Elephas - Deep Learning sobre Spark\n",
    "from elephas.spark_model import SparkModel\n",
    "from elephas.utils.rdd_utils import to_simple_rdd\n",
    "\n",
    "print(\"✓ Librerías importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spark_config",
   "metadata": {},
   "source": [
    "## 2. Configurar Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spark_session",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RedNeuronal_Elephas\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"✓ Spark Session creada\")\n",
    "print(f\"  Versión Spark: {spark.version}\")\n",
    "print(f\"  Master: {sc.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_generation",
   "metadata": {},
   "source": [
    "## 3. Generar Dataset Sintético (Big Data)\n",
    "Generamos 100,000 registros con 10 features para demostrar capacidades de Big Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros del dataset\n",
    "NUM_RECORDS = 100000  # 100k registros para simular big data\n",
    "NUM_FEATURES = 10     # 10 características de entrada\n",
    "\n",
    "# Generar datos distribuidos con Spark\n",
    "# Creamos un DataFrame con features aleatorias y target binario\n",
    "data_rdd = sc.parallelize(range(NUM_RECORDS), numSlices=8)\n",
    "\n",
    "def generate_row(idx):\n",
    "    \"\"\"Genera una fila con features aleatorias y target\"\"\"\n",
    "    import random\n",
    "    random.seed(idx)  # Seed para reproducibilidad\n",
    "    \n",
    "    # Generar 10 features aleatorias entre 0 y 1\n",
    "    features = [random.random() for _ in range(NUM_FEATURES)]\n",
    "    \n",
    "    # Target: clasificación binaria basada en suma de features\n",
    "    # Si suma > 5, target=1, sino target=0\n",
    "    target = 1.0 if sum(features) > 5.0 else 0.0\n",
    "    \n",
    "    return tuple(features + [target])\n",
    "\n",
    "# Generar datos\n",
    "generated_rdd = data_rdd.map(generate_row)\n",
    "\n",
    "# Crear DataFrame\n",
    "columns = [f\"feature_{i}\" for i in range(NUM_FEATURES)] + [\"target\"]\n",
    "df = generated_rdd.toDF(columns)\n",
    "\n",
    "print(f\"✓ Dataset generado: {df.count():,} registros x {NUM_FEATURES} features\")\n",
    "print(f\"\\nPrimeras 5 filas:\")\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing",
   "metadata": {},
   "source": [
    "## 4. Preprocesamiento de Datos con Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocess",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensamblar features en un vector\n",
    "feature_columns = [f\"feature_{i}\" for i in range(NUM_FEATURES)]\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_columns,\n",
    "    outputCol=\"features_raw\"\n",
    ")\n",
    "\n",
    "df_assembled = assembler.transform(df)\n",
    "\n",
    "# Normalizar features con StandardScaler de Spark ML\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "df_scaled = scaler_model.transform(df_assembled)\n",
    "\n",
    "# Seleccionar solo features y target\n",
    "df_final = df_scaled.select(\"features\", \"target\")\n",
    "\n",
    "print(\"✓ Datos preprocesados y normalizados\")\n",
    "df_final.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split_data",
   "metadata": {},
   "source": [
    "## 5. Dividir Datos en Train/Test (80/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_test_split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 80% train, 20% test\n",
    "train_df, test_df = df_final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "train_df = train_df.cache()\n",
    "test_df = test_df.cache()\n",
    "\n",
    "train_count = train_df.count()\n",
    "test_count = test_df.count()\n",
    "\n",
    "print(f\"✓ Datos divididos:\")\n",
    "print(f\"  Train: {train_count:,} registros\")\n",
    "print(f\"  Test:  {test_count:,} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_architecture",
   "metadata": {},
   "source": [
    "## 6. Definir Arquitectura de Red Neuronal\n",
    "### Especificaciones:\n",
    "- **Capa oculta**: 4 neuronas, activación sigmoid\n",
    "- **Capa salida**: 1 neurona, activación sigmoid\n",
    "- **Función de pérdida**: MAE (Mean Absolute Error)\n",
    "- **Optimizador**: Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"Crea el modelo de red neuronal según especificaciones\"\"\"\n",
    "    model = Sequential([\n",
    "        # Capa oculta: 4 neuronas, activación sigmoid\n",
    "        Dense(\n",
    "            units=4,\n",
    "            activation='sigmoid',\n",
    "            input_shape=(NUM_FEATURES,),\n",
    "            name='capa_oculta'\n",
    "        ),\n",
    "        \n",
    "        # Capa de salida: 1 neurona, activación sigmoid\n",
    "        Dense(\n",
    "            units=1,\n",
    "            activation='sigmoid',\n",
    "            name='capa_salida'\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    # Compilar con MAE como función de pérdida\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mean_absolute_error',  # MAE\n",
    "        metrics=['mae', 'accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear y mostrar el modelo\n",
    "model = create_model()\n",
    "print(\"✓ Modelo creado\")\n",
    "print(\"\\nArquitectura del modelo:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepare_rdd",
   "metadata": {},
   "source": [
    "## 7. Preparar RDD para Elephas\n",
    "Convertimos el DataFrame de Spark a formato RDD compatible con Elephas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convert_rdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_df_to_rdd(df):\n",
    "    \"\"\"Convierte Spark DataFrame a RDD en formato (features_array, label)\"\"\"\n",
    "    def row_to_tuple(row):\n",
    "        # Convertir DenseVector a lista\n",
    "        features_list = row.features.toArray().tolist()\n",
    "        label = float(row.target)\n",
    "        return (features_list, label)\n",
    "    \n",
    "    return df.rdd.map(row_to_tuple)\n",
    "\n",
    "# Convertir train y test a RDD\n",
    "train_rdd = spark_df_to_rdd(train_df)\n",
    "test_rdd = spark_df_to_rdd(test_df)\n",
    "\n",
    "print(\"✓ DataFrames convertidos a RDD\")\n",
    "print(f\"  Train RDD: {train_rdd.count():,} registros\")\n",
    "print(f\"  Test RDD: {test_rdd.count():,} registros\")\n",
    "print(f\"\\nEjemplo de dato en RDD:\")\n",
    "print(train_rdd.take(1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elephas_training",
   "metadata": {},
   "source": [
    "## 8. Entrenar Modelo con Elephas (Distribuido)\n",
    "### Parámetros:\n",
    "- **Minibatch size**: 1000 (según especificación)\n",
    "- **Epochs**: 5\n",
    "- **Mode**: 'asynchronous' (entrenamiento asíncrono distribuido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elephas_train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de Elephas\n",
    "BATCH_SIZE = 1000  # Minibatch de 1000 según especificación\n",
    "EPOCHS = 5\n",
    "\n",
    "# Crear SparkModel de Elephas\n",
    "spark_model = SparkModel(\n",
    "    model=model,\n",
    "    frequency='epoch',      # Sincronización por epoch\n",
    "    mode='asynchronous',    # Modo asíncrono para mejor performance\n",
    "    num_workers=4           # Número de workers (ajustar según tu sistema)\n",
    ")\n",
    "\n",
    "print(\"✓ SparkModel de Elephas creado\")\n",
    "print(f\"\\nIniciando entrenamiento distribuido...\")\n",
    "print(f\"  Batch size: {BATCH_SIZE:,}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Mode: asynchronous\")\n",
    "print(f\"  Registros de entrenamiento: {train_rdd.count():,}\\n\")\n",
    "\n",
    "# Entrenar el modelo de forma distribuida\n",
    "spark_model.fit(\n",
    "    train_rdd,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1,\n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Entrenamiento completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation",
   "metadata": {},
   "source": [
    "## 9. Evaluar Modelo en Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener predicciones en test set\n",
    "predictions_rdd = spark_model.predict(test_rdd)\n",
    "\n",
    "# Calcular métricas\n",
    "def calculate_metrics(predictions_rdd, test_rdd):\n",
    "    \"\"\"Calcula MAE y Accuracy\"\"\"\n",
    "    # Combinar predicciones con valores reales\n",
    "    predictions_list = predictions_rdd.collect()\n",
    "    actuals_list = test_rdd.map(lambda x: x[1]).collect()\n",
    "    \n",
    "    # Calcular MAE\n",
    "    mae = sum(abs(p - a) for p, a in zip(predictions_list, actuals_list)) / len(predictions_list)\n",
    "    \n",
    "    # Calcular Accuracy (clasificación binaria con threshold 0.5)\n",
    "    correct = sum(1 for p, a in zip(predictions_list, actuals_list) \n",
    "                  if (p >= 0.5 and a == 1.0) or (p < 0.5 and a == 0.0))\n",
    "    accuracy = correct / len(predictions_list)\n",
    "    \n",
    "    return mae, accuracy\n",
    "\n",
    "mae, accuracy = calculate_metrics(predictions_rdd, test_rdd)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RESULTADOS DE EVALUACIÓN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Registros evaluados: {test_rdd.count():,}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predictions_sample",
   "metadata": {},
   "source": [
    "## 10. Mostrar Ejemplos de Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show_predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomar 10 ejemplos de predicciones\n",
    "sample_test = test_rdd.take(10)\n",
    "sample_predictions = predictions_rdd.take(10)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EJEMPLOS DE PREDICCIONES\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'#':<5} {'Predicción':<15} {'Real':<10} {'Error Absoluto':<15} {'Correcto':<10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i, (test_sample, pred) in enumerate(zip(sample_test, sample_predictions), 1):\n",
    "    actual = test_sample[1]\n",
    "    pred_class = 1 if pred >= 0.5 else 0\n",
    "    actual_class = int(actual)\n",
    "    error = abs(pred - actual)\n",
    "    correct = \"✓\" if pred_class == actual_class else \"✗\"\n",
    "    \n",
    "    print(f\"{i:<5} {pred:.4f} ({pred_class})  {actual:.1f} ({actual_class})  \"\n",
    "          f\"{error:>8.4f}       {correct:^10}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_summary",
   "metadata": {},
   "source": [
    "## 11. Resumen Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN DE IMPLEMENTACIÓN\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n✓ ARQUITECTURA:\")\n",
    "print(f\"  - Capa oculta: 4 neuronas, activación sigmoid\")\n",
    "print(f\"  - Capa salida: 1 neurona, activación sigmoid\")\n",
    "print(f\"  - Total parámetros: {model.count_params():,}\")\n",
    "\n",
    "print(\"\\n✓ CONFIGURACIÓN:\")\n",
    "print(f\"  - Función de pérdida: MAE (Mean Absolute Error)\")\n",
    "print(f\"  - Optimizador: Adam\")\n",
    "print(f\"  - Minibatch size: {BATCH_SIZE:,}\")\n",
    "print(f\"  - Epochs: {EPOCHS}\")\n",
    "\n",
    "print(\"\\n✓ DATOS:\")\n",
    "print(f\"  - Total registros: {NUM_RECORDS:,}\")\n",
    "print(f\"  - Features: {NUM_FEATURES}\")\n",
    "print(f\"  - Train: {train_count:,} registros\")\n",
    "print(f\"  - Test: {test_count:,} registros\")\n",
    "\n",
    "print(\"\\n✓ RESULTADOS:\")\n",
    "print(f\"  - MAE en Test: {mae:.4f}\")\n",
    "print(f\"  - Accuracy en Test: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n✓ TECNOLOGÍAS:\")\n",
    "print(f\"  - PySpark (procesamiento distribuido)\")\n",
    "print(f\"  - Elephas (deep learning distribuido)\")\n",
    "print(f\"  - Keras/TensorFlow (modelo de red neuronal)\")\n",
    "print(f\"  - 100% Big Data pipeline\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "## 12. Limpiar Recursos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stop_spark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpersist cached DataFrames\n",
    "train_df.unpersist()\n",
    "test_df.unpersist()\n",
    "\n",
    "# Detener Spark\n",
    "spark.stop()\n",
    "print(\"✓ Recursos liberados y Spark detenido\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
