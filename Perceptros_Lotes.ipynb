{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdrJOO0vZBGtyNoEmuN741",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gus-oss/DeepLearning/blob/main/Perceptros_Lotes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vL5OV_beAGRI",
        "outputId": "80b42f36-b0ae-4266-ebdd-64e7d26a7b20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Librerias importantes\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#Coneccion con google drive\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "#Importar la carpeta de drive\n",
        "drive.mount('/content/drive') #coneccion con drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"/content/drive/MyDrive/DeepLearning/archivos/EarlyRetirementPrediction.csv.xlsx\")"
      ],
      "metadata": {
        "id": "WwwUf6mBAgCB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = StandardScaler() # Standardisación\n",
        "scaled = sc.fit_transform(df)\n",
        "res = pd.DataFrame(scaled, columns=df.columns) #este es el df que utilizaremos"
      ],
      "metadata": {
        "id": "VYbYI-tfAaoi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#utilizaremos el conjunto res, puesto ya esta estandarizado\n",
        "X_columns = ['Gender', 'Desease', 'Education Level', 'Marital Status',\n",
        "             'Monthly Income', 'Employee/Employer', 'Dependants',\n",
        "             'Unemployment Rate', 'Stock Market', 'Credit Score',\n",
        "             'Government Bonds Return']\n",
        "X_raw = res[X_columns].values\n",
        "y = df['Retire Before 65 Years Old'].values  # Tomamos y del conjnto original y no del estandarizado"
      ],
      "metadata": {
        "id": "GnQwfbfhAv62"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#declaramos las funciones de la simoide, la funcion de costos y el gradiente\n",
        "def sigmoide(z):\n",
        "    z = np.clip(z, -500, 500)#Evita overflow\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "\n",
        "def cross_entropy_loss(y, y_hat):\n",
        "    epsilon = 1e-15 #para evitar el cero\n",
        "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
        "\n",
        "    loss = -np.sum(y * np.log(y_hat))\n",
        "    return loss\n",
        "\n",
        "def gradientes_cross_entropy(X, y, y_hat):\n",
        "    dL_dz = - y * (1 - y_hat) # dL/dz para cada observación\n",
        "    grad_w = X.T @ dL_dz #gradientes\n",
        "    grad_b = np.sum(dL_dz)\n",
        "\n",
        "    return grad_w, grad_b"
      ],
      "metadata": {
        "id": "97voNQFqA3D2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#formamos una funcion para la propagacion hacia adelante\n",
        "def forward_pass(X, w, b):\n",
        "    z = X @ w + b\n",
        "    y_hat = sigmoide(z)\n",
        "    return z, y_hat"
      ],
      "metadata": {
        "id": "Pd_gDgZlBLmo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicialización\n",
        "n_features = X_raw.shape[1]\n",
        "N = X_raw.shape[0]\n",
        "\n",
        "np.random.seed(42)\n",
        "w = np.random.randn(n_features) * 0.1\n",
        "b = 0.0"
      ],
      "metadata": {
        "id": "PDPe4yAjBUgZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizamos el entrenamiento por lotes\n",
        "batch_size = 100\n",
        "eta = 0.01\n",
        "n_batches = N // batch_size\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"ENTRENAMIENTO POR LOTES (batch_size = {batch_size})\")\n",
        "print(f\"Número de lotes: {n_batches}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "for lote in range(n_batches):\n",
        "    inicio = lote * batch_size\n",
        "    fin = inicio + batch_size\n",
        "\n",
        "    X_batch = X_raw[inicio:fin]\n",
        "    y_batch = y[inicio:fin]\n",
        "\n",
        "    z_batch, y_hat_batch = forward_pass(X_batch, w, b)\n",
        "    L_batch = cross_entropy_loss(y_batch, y_hat_batch)\n",
        "\n",
        "    grad_w, grad_b = gradientes_cross_entropy(X_batch, y_batch, y_hat_batch)\n",
        "\n",
        "    w = w - eta * grad_w\n",
        "    b = b - eta * grad_b\n",
        "\n",
        "    print(f\"Lote {lote+1:2d}/{n_batches} | Índices [{inicio:4d}:{fin:4d}] | Loss lote: {L_batch:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Sz0-RfuBeyD",
        "outputId": "4087cf8b-77ec-4ef1-e401-1f03b4dad2c0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ENTRENAMIENTO POR LOTES (batch_size = 100)\n",
            "Número de lotes: 15\n",
            "============================================================\n",
            "Lote  1/15 | Índices [   0: 100] | Loss lote: 30.9147\n",
            "Lote  2/15 | Índices [ 100: 200] | Loss lote: 20.5003\n",
            "Lote  3/15 | Índices [ 200: 300] | Loss lote: 18.1646\n",
            "Lote  4/15 | Índices [ 300: 400] | Loss lote: 14.8295\n",
            "Lote  5/15 | Índices [ 400: 500] | Loss lote: 12.0606\n",
            "Lote  6/15 | Índices [ 500: 600] | Loss lote: 12.0543\n",
            "Lote  7/15 | Índices [ 600: 700] | Loss lote: 8.8474\n",
            "Lote  8/15 | Índices [ 700: 800] | Loss lote: 7.8111\n",
            "Lote  9/15 | Índices [ 800: 900] | Loss lote: 7.0451\n",
            "Lote 10/15 | Índices [ 900:1000] | Loss lote: 6.3738\n",
            "Lote 11/15 | Índices [1000:1100] | Loss lote: 7.1667\n",
            "Lote 12/15 | Índices [1100:1200] | Loss lote: 16.6758\n",
            "Lote 13/15 | Índices [1200:1300] | Loss lote: 11.3502\n",
            "Lote 14/15 | Índices [1300:1400] | Loss lote: 11.7678\n",
            "Lote 15/15 | Índices [1400:1500] | Loss lote: 13.1425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el primer lote, se tuvo un error de 30.917, mientras que en el ultimo lote se tuvo un error de 13.1425. Hay que mencionar qu edel lote q hasta el 10 la funcion de costos disminuyo, pero en los lostes de 12-15 aumento."
      ],
      "metadata": {
        "id": "ihmnDjxoCJ27"
      }
    }
  ]
}